"""
Video processing service for PAPI measurements
Enhanced with computer vision techniques from the prototype
"""
import cv2
import numpy as np
from typing import Dict, List, Tuple, Optional
import os
import json
import logging
from dataclasses import dataclass
import math
from datetime import datetime
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo
import subprocess
import struct
import re
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor
import threading

logger = logging.getLogger(__name__)


class GPUAccelerator:
    """GPU acceleration utilities for OpenCV operations"""
    
    def __init__(self):
        self.gpu_enabled = False
        self.opencl_available = False
        self._initialize_gpu()
    
    def _initialize_gpu(self):
        """Initialize GPU acceleration if available"""
        try:
            # Check OpenCL availability (works on M1/M2/M3 Macs)
            if hasattr(cv2.ocl, 'haveOpenCL') and cv2.ocl.haveOpenCL():
                cv2.ocl.setUseOpenCL(True)
                self.opencl_available = cv2.ocl.useOpenCL()
                if self.opencl_available:
                    device = cv2.ocl.Device.getDefault()
                    logger.info(f"OpenCL GPU acceleration enabled: {device.name()} ({device.maxComputeUnits()} cores)")
                    self.gpu_enabled = True
                else:
                    logger.warning("OpenCL available but failed to enable")
            else:
                logger.info("OpenCL not available - using CPU processing")
                
        except Exception as e:
            logger.warning(f"GPU initialization failed: {e}")
            self.gpu_enabled = False
    
    def is_enabled(self) -> bool:
        """Check if GPU acceleration is enabled"""
        return self.gpu_enabled
    
    def cvtColor_gpu(self, src, code):
        """GPU-accelerated color space conversion"""
        if self.gpu_enabled:
            try:
                # Upload to GPU, process, download
                gpu_src = cv2.UMat(src)
                gpu_dst = cv2.cvtColor(gpu_src, code)
                return gpu_dst.get()
            except:
                pass
        # Fallback to CPU
        return cv2.cvtColor(src, code)
    
    def threshold_gpu(self, src, thresh, maxval, type):
        """GPU-accelerated thresholding"""
        if self.gpu_enabled:
            try:
                gpu_src = cv2.UMat(src)
                _, gpu_dst = cv2.threshold(gpu_src, thresh, maxval, type)
                return gpu_dst.get()
            except:
                pass
        # Fallback to CPU
        _, dst = cv2.threshold(src, thresh, maxval, type)
        return dst
    
    def morphologyEx_gpu(self, src, op, kernel, iterations=1):
        """GPU-accelerated morphological operations"""
        if self.gpu_enabled:
            try:
                gpu_src = cv2.UMat(src)
                gpu_dst = cv2.morphologyEx(gpu_src, op, kernel, iterations=iterations)
                return gpu_dst.get()
            except:
                pass
        # Fallback to CPU
        return cv2.morphologyEx(src, op, kernel, iterations=iterations)
    
    def bitwise_or_gpu(self, src1, src2):
        """GPU-accelerated bitwise OR"""
        if self.gpu_enabled:
            try:
                gpu_src1 = cv2.UMat(src1)
                gpu_src2 = cv2.UMat(src2)
                gpu_dst = cv2.bitwise_or(gpu_src1, gpu_src2)
                return gpu_dst.get()
            except:
                pass
        # Fallback to CPU
        return cv2.bitwise_or(src1, src2)
    
    def resize_gpu(self, src, size, interpolation=cv2.INTER_LINEAR):
        """GPU-accelerated resize"""
        if self.gpu_enabled:
            try:
                gpu_src = cv2.UMat(src)
                gpu_dst = cv2.resize(gpu_src, size, interpolation=interpolation)
                return gpu_dst.get()
            except:
                pass
        # Fallback to CPU
        return cv2.resize(src, size, interpolation=interpolation)


class FrameProcessingCache:
    """Cache for expensive operations to avoid recomputation"""
    
    def __init__(self, max_cache_size: int = 100):
        self.gps_cache = {}  # frame_number -> GPSData
        self.processed_frames = {}  # frame_number -> processed_data
        self.max_cache_size = max_cache_size
        self._lock = threading.Lock()
    
    def get_gps_data(self, frame_number: int) -> Optional['GPSData']:
        """Get cached GPS data for frame"""
        with self._lock:
            return self.gps_cache.get(frame_number)
    
    def set_gps_data(self, frame_number: int, gps_data: 'GPSData'):
        """Cache GPS data for frame"""
        with self._lock:
            if len(self.gps_cache) >= self.max_cache_size:
                # Remove oldest entry
                oldest = min(self.gps_cache.keys())
                del self.gps_cache[oldest]
            self.gps_cache[frame_number] = gps_data
    
    def clear(self):
        """Clear all cached data"""
        with self._lock:
            self.gps_cache.clear()
            self.processed_frames.clear()


class BatchFrameProcessor:
    """Process frames in batches for better GPU utilization"""
    
    def __init__(self, batch_size: int = 8, num_workers: int = 2):
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.gpu_accelerator = GPUAccelerator()
        self.cache = FrameProcessingCache()
    
    def preprocess_frames_batch(self, frames: List[np.ndarray]) -> List[Tuple[np.ndarray, np.ndarray]]:
        """Preprocess a batch of frames for light detection"""
        results = []
        
        if self.gpu_accelerator.is_enabled():
            # GPU batch processing
            for frame in frames:
                # Convert to HSV using GPU
                hsv = self.gpu_accelerator.cvtColor_gpu(frame, cv2.COLOR_BGR2HSV)
                value_channel = hsv[:, :, 2]
                
                # Create brightness mask using GPU
                bright_mask = self.gpu_accelerator.threshold_gpu(value_channel, 150, 255, cv2.THRESH_BINARY)
                
                results.append((bright_mask, value_channel))
        else:
            # CPU batch processing with threading
            def process_single_frame(frame):
                hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
                value_channel = hsv[:, :, 2]
                _, bright_mask = cv2.threshold(value_channel, 150, 255, cv2.THRESH_BINARY)
                return (bright_mask, value_channel)
            
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                results = list(executor.map(process_single_frame, frames))
        
        return results


@dataclass
class DetectedLight:
    """Represents a detected light object"""
    x: float
    y: float
    width: float
    height: float
    confidence: float
    class_name: str
    brightness: float
    r: int
    g: int
    b: int
    frame_num: int = 0
    timestamp_ms: float = 0.0
    intensity: float = 0.0
    # GPS and drone data
    drone_latitude: Optional[float] = None
    drone_longitude: Optional[float] = None
    drone_altitude: Optional[float] = None
    # PAPI light reference position
    papi_latitude: Optional[float] = None
    papi_longitude: Optional[float] = None
    papi_elevation: Optional[float] = None


@dataclass
class GPSData:
    """Represents GPS data for a specific timestamp/frame"""
    timestamp_ms: float
    latitude: float
    longitude: float
    altitude: float  # meters above sea level
    speed: Optional[float] = None  # m/s
    heading: Optional[float] = None  # degrees
    satellites: Optional[int] = None
    accuracy: Optional[float] = None  # meters
    frame_number: Optional[int] = None
    
    def to_dict(self) -> Dict:
        """Convert to dictionary for JSON serialization"""
        return {
            'timestamp_ms': self.timestamp_ms,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'altitude': self.altitude,
            'speed': self.speed,
            'heading': self.heading,
            'satellites': self.satellites,
            'accuracy': self.accuracy,
            'frame_number': self.frame_number
        }


class GPSExtractor:
    """Extract GPS metadata from drone video files"""
    
    def __init__(self):
        self.supported_formats = ['.mp4', '.mov', '.avi', '.mkv']
        
    def extract_gps_data(self, video_path: str) -> List[GPSData]:
        """
        Extract GPS data from video file using multiple methods.
        Tries different extraction methods in order of reliability.
        """
        video_path_obj = Path(video_path)
        if not video_path_obj.exists():
            logger.error(f"Video file not found: {video_path}")
            return []
        
        logger.info(f"Extracting GPS data from: {video_path}")
        
        # Method 1: Try DJI SRT file first (most reliable for DJI drones)
        logger.debug("Trying DJI SRT extraction...")
        try:
            gps_data = self._extract_from_dji_srt(video_path_obj)
            if gps_data:
                logger.info(f"Extracted {len(gps_data)} GPS points from DJI SRT file")
                return gps_data
        except Exception as e:
            logger.debug(f"DJI SRT extraction failed: {e}")
        
        # Method 2: Try exiftool for DJI embedded frame metadata (most accurate)
        logger.debug("Trying exiftool frame extraction...")
        try:
            gps_data = self._extract_with_exiftool_frames(video_path_obj)
            if gps_data:
                logger.info(f"Extracted {len(gps_data)} GPS points using exiftool frame data")
                return gps_data
        except Exception as e:
            logger.debug(f"exiftool frame extraction failed: {e}")
        
        # Method 3: Try ffprobe for standard GPS metadata
        logger.debug("Trying ffprobe extraction...")
        try:
            gps_data = self._extract_with_ffprobe(video_path_obj)
            if gps_data:
                logger.info(f"Extracted {len(gps_data)} GPS points using ffprobe")
                return gps_data
        except Exception as e:
            logger.debug(f"ffprobe extraction failed: {e}")
        
        logger.warning(f"No GPS data found in {video_path} - will use fallback data")
        return []
    
    def _extract_from_dji_srt(self, video_path: Path) -> List[GPSData]:
        """Extract GPS data from DJI SRT subtitle file"""
        srt_path = video_path.with_suffix('.SRT')
        if not srt_path.exists():
            srt_path = video_path.with_suffix('.srt')
        
        if not srt_path.exists():
            return []
        
        try:
            gps_data = []
            with open(srt_path, 'r') as f:
                content = f.read()
            
            # Parse SRT entries
            entries = content.strip().split('\n\n')
            for entry in entries:
                lines = entry.strip().split('\n')
                if len(lines) < 3:
                    continue
                
                # Parse timestamp
                timestamp_line = lines[1]
                timestamp_ms = self._parse_srt_timestamp(timestamp_line)
                
                # Parse GPS data from subtitle text
                text = ' '.join(lines[2:])
                gps_point = self._parse_dji_srt_text(text, timestamp_ms)
                if gps_point:
                    gps_data.append(gps_point)
            
            return gps_data
            
        except Exception as e:
            logger.debug(f"DJI SRT extraction failed: {e}")
            return []
    
    def _extract_with_exiftool_frames(self, video_path: Path) -> List[GPSData]:
        """Extract GPS data from DJI embedded frame metadata using exiftool"""
        try:
            # Check if exiftool is available
            check_cmd = ['which', 'exiftool']
            check_result = subprocess.run(check_cmd, capture_output=True)
            if check_result.returncode != 0:
                logger.debug("exiftool not found, skipping this method")
                return []
            
            # Extract embedded frame GPS data (DJI format)
            cmd = ['exiftool', '-ee', '-G', '-a', str(video_path)]
            logger.debug("Extracting embedded GPS with exiftool -ee...")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0 and result.stdout:
                # Parse DJI embedded frame GPS data
                gps_data = []
                for line in result.stdout.split('\n'):
                    if '[QuickTime]     Text' in line and 'FrameCnt:' in line:
                        # Parse frame metadata
                        gps_point = self._parse_dji_frame_metadata(line)
                        if gps_point:
                            gps_data.append(gps_point)
                
                if gps_data:
                    logger.info(f"Found {len(gps_data)} GPS points in DJI embedded metadata")
                    return gps_data
            
            return []
            
        except Exception as e:
            logger.debug(f"exiftool frame extraction failed: {e}")
            return []
    
    def _extract_with_ffprobe(self, video_path: Path) -> List[GPSData]:
        """Extract GPS data using ffprobe"""
        try:
            # Get metadata streams
            cmd = [
                'ffprobe', '-v', 'quiet',
                '-print_format', 'json',
                '-show_streams',
                '-show_format',
                str(video_path)
            ]
            
            logger.debug(f"Running ffprobe command: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                return []
            
            data = json.loads(result.stdout)
            
            # Check format tags for GPS coordinates
            format_tags = data.get('format', {}).get('tags', {})
            if 'location' in format_tags or 'com.apple.quicktime.location.ISO6709' in format_tags:
                # Parse static GPS location
                location = format_tags.get('location') or format_tags.get('com.apple.quicktime.location.ISO6709')
                gps_point = self._parse_iso6709(location)
                if gps_point:
                    # For static GPS, create one point at the beginning
                    return [gps_point]
            
            return []
            
        except Exception as e:
            logger.debug(f"ffprobe extraction failed: {e}")
            return []
    
    def _parse_srt_timestamp(self, timestamp_str: str) -> float:
        """Parse SRT timestamp to milliseconds"""
        try:
            # Format: 00:00:12,000 --> 00:00:15,000
            start_time = timestamp_str.split(' --> ')[0]
            time_parts = re.match(r'(\d+):(\d+):(\d+),(\d+)', start_time)
            if time_parts:
                hours = int(time_parts.group(1))
                minutes = int(time_parts.group(2))
                seconds = int(time_parts.group(3))
                millis = int(time_parts.group(4))
                
                total_ms = (hours * 3600 + minutes * 60 + seconds) * 1000 + millis
                return total_ms
        except:
            pass
        return 0
    
    def _parse_dji_srt_text(self, text: str, timestamp_ms: float) -> Optional[GPSData]:
        """Parse DJI drone SRT subtitle text"""
        try:
            # DJI format includes GPS coordinates and telemetry
            # Example: "GPS (40.7580, -73.9855, 15) [12]"
            gps_pattern = r'GPS\s*\(([+-]?\d+\.?\d*),\s*([+-]?\d+\.?\d*),\s*([+-]?\d+\.?\d*)\)'
            match = re.search(gps_pattern, text)
            if match:
                lat = float(match.group(1))
                lon = float(match.group(2))
                alt = float(match.group(3))
                
                # Try to extract additional data
                satellites = None
                sat_pattern = r'\[(\d+)\]'
                sat_match = re.search(sat_pattern, text)
                if sat_match:
                    satellites = int(sat_match.group(1))
                
                return GPSData(
                    timestamp_ms=timestamp_ms,
                    latitude=lat,
                    longitude=lon,
                    altitude=alt,
                    satellites=satellites
                )
        except:
            pass
        return None
    
    def _parse_dji_frame_metadata(self, line: str) -> Optional[GPSData]:
        """Parse DJI embedded frame GPS metadata from exiftool output"""
        try:
            # Extract frame number
            frame_match = re.search(r'FrameCnt:\s*(\d+)', line)
            if not frame_match:
                return None
            
            frame_num = int(frame_match.group(1))
            
            # Extract timestamp
            timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+)', line)
            if timestamp_match:
                # Convert timestamp to milliseconds
                dt = datetime.strptime(timestamp_match.group(1)[:19], '%Y-%m-%d %H:%M:%S')
                ms = int(timestamp_match.group(1).split('.')[-1][:3]) if '.' in timestamp_match.group(1) else 0
                timestamp_ms = (dt.timestamp() * 1000) + ms
            else:
                # Approximate based on frame number (assuming 30fps)
                timestamp_ms = frame_num * 33.33
            
            # Extract GPS coordinates
            lat_match = re.search(r'\[latitude:\s*([-\d.]+)\]', line)
            lon_match = re.search(r'\[longitude:\s*([-\d.]+)\]', line)
            
            if not lat_match or not lon_match:
                return None
            
            latitude = float(lat_match.group(1))
            longitude = float(lon_match.group(1))
            
            # Extract altitudes
            rel_alt_match = re.search(r'\[rel_alt:\s*([-\d.]+)', line)
            abs_alt_match = re.search(r'abs_alt:\s*([-\d.]+)\]', line)
            
            altitude = float(abs_alt_match.group(1)) if abs_alt_match else (
                float(rel_alt_match.group(1)) if rel_alt_match else 0
            )
            
            # Extract gimbal yaw as heading
            yaw_match = re.search(r'\[gb_yaw:\s*([-\d.]+)', line)
            heading = float(yaw_match.group(1)) if yaw_match else None
            
            return GPSData(
                timestamp_ms=timestamp_ms,
                latitude=latitude,
                longitude=longitude,
                altitude=altitude,
                heading=heading,
                frame_number=frame_num
            )
            
        except Exception as e:
            logger.debug(f"Failed to parse DJI frame metadata: {e}")
            return None
    
    def _parse_iso6709(self, location_str: str) -> Optional[GPSData]:
        """Parse ISO 6709 location string"""
        try:
            # Format: +40.7580-073.9855+011.234/
            pattern = r'([+-]\d+\.\d+)([+-]\d+\.\d+)([+-]\d+\.\d+)?'
            match = re.match(pattern, location_str)
            if match:
                lat = float(match.group(1))
                lon = float(match.group(2))
                alt = float(match.group(3)) if match.group(3) else 0
                
                return GPSData(
                    timestamp_ms=0,
                    latitude=lat,
                    longitude=lon,
                    altitude=alt
                )
        except:
            pass
        return None
    
    def interpolate_gps_for_frame(self, gps_data: List[GPSData], frame_num: int, fps: float) -> Optional[GPSData]:
        """
        Interpolate GPS position for a specific frame number.
        
        Args:
            gps_data: List of GPS data points
            frame_num: Frame number to interpolate for
            fps: Frames per second of the video
        
        Returns:
            Interpolated GPS data for the frame, or None if not available
        """
        if not gps_data:
            return None
        
        # Calculate timestamp for this frame
        target_timestamp_ms = (frame_num / fps) * 1000
        
        # If only one GPS point, return it for all frames
        if len(gps_data) == 1:
            gps_point = gps_data[0]
            return GPSData(
                timestamp_ms=target_timestamp_ms,
                latitude=gps_point.latitude,
                longitude=gps_point.longitude,
                altitude=gps_point.altitude,
                speed=gps_point.speed,
                heading=gps_point.heading,
                satellites=gps_point.satellites,
                accuracy=gps_point.accuracy,
                frame_number=frame_num
            )
        
        # Check if GPS data has frame numbers - use those for more accurate interpolation
        has_frame_numbers = all(p.frame_number is not None for p in gps_data)
        
        if has_frame_numbers:
            # Use frame numbers for interpolation (more accurate for DJI videos)
            before_point = None
            after_point = None
            
            for i, gps_point in enumerate(gps_data):
                if gps_point.frame_number <= frame_num:
                    before_point = gps_point
                if gps_point.frame_number >= frame_num and after_point is None:
                    after_point = gps_point
                    break
        else:
            # Fall back to timestamp-based interpolation
            # Find surrounding GPS points for interpolation
            before_point = None
            after_point = None
            
            for i, gps_point in enumerate(gps_data):
                if gps_point.timestamp_ms <= target_timestamp_ms:
                    before_point = gps_point
                if gps_point.timestamp_ms >= target_timestamp_ms and after_point is None:
                    after_point = gps_point
                    break
        
        # If exact match found (check frame number)
        if before_point and has_frame_numbers and before_point.frame_number == frame_num:
            return GPSData(
                timestamp_ms=target_timestamp_ms,
                latitude=before_point.latitude,
                longitude=before_point.longitude,
                altitude=before_point.altitude,
                speed=before_point.speed,
                heading=before_point.heading,
                satellites=before_point.satellites,
                accuracy=before_point.accuracy,
                frame_number=frame_num
            )
        
        # If we have both points, interpolate
        if before_point and after_point:
            # Calculate interpolation factor
            if has_frame_numbers:
                # Use frame numbers for interpolation
                frame_diff = after_point.frame_number - before_point.frame_number
                if frame_diff > 0:
                    factor = (frame_num - before_point.frame_number) / frame_diff
                else:
                    factor = 0
            else:
                # Use timestamps for interpolation
                time_diff = after_point.timestamp_ms - before_point.timestamp_ms
                if time_diff > 0:
                    factor = (target_timestamp_ms - before_point.timestamp_ms) / time_diff
                else:
                    factor = 0
            
            # Linear interpolation
            lat = before_point.latitude + (after_point.latitude - before_point.latitude) * factor
            lon = before_point.longitude + (after_point.longitude - before_point.longitude) * factor
            alt = before_point.altitude + (after_point.altitude - before_point.altitude) * factor
            
            # Interpolate optional fields
            speed = None
            if before_point.speed is not None and after_point.speed is not None:
                speed = before_point.speed + (after_point.speed - before_point.speed) * factor
            
            heading = None
            if before_point.heading is not None and after_point.heading is not None:
                # Circular interpolation for heading
                h1 = before_point.heading
                h2 = after_point.heading
                diff = (h2 - h1 + 180) % 360 - 180
                heading = (h1 + diff * factor) % 360
            
            return GPSData(
                timestamp_ms=target_timestamp_ms,
                latitude=lat,
                longitude=lon,
                altitude=alt,
                speed=speed,
                heading=heading,
                satellites=before_point.satellites,  # Use last known value
                accuracy=before_point.accuracy,  # Use last known value
                frame_number=frame_num
            )
        
        # If only before point available (extrapolate forward)
        if before_point:
            return GPSData(
                timestamp_ms=target_timestamp_ms,
                latitude=before_point.latitude,
                longitude=before_point.longitude,
                altitude=before_point.altitude,
                speed=before_point.speed,
                heading=before_point.heading,
                satellites=before_point.satellites,
                accuracy=before_point.accuracy,
                frame_number=frame_num
            )
        
        # If only after point available (extrapolate backward)
        if after_point:
            return GPSData(
                timestamp_ms=target_timestamp_ms,
                latitude=after_point.latitude,
                longitude=after_point.longitude,
                altitude=after_point.altitude,
                speed=after_point.speed,
                heading=after_point.heading,
                satellites=after_point.satellites,
                accuracy=after_point.accuracy,
                frame_number=frame_num
            )
        
        return None


class RunwayLightDetector:
    """Advanced light detection using computer vision techniques from prototype with GPU acceleration"""
    
    def __init__(self, brightness_threshold: int = 150, 
                 min_area: int = 5, 
                 max_area: int = 5000,
                 saturation_threshold: int = 240,
                 use_gpu: bool = True):
        self.brightness_threshold = brightness_threshold
        self.min_area = min_area
        self.max_area = max_area
        self.saturation_threshold = saturation_threshold
        
        # Initialize GPU acceleration
        self.gpu_accelerator = GPUAccelerator() if use_gpu else None
        if self.gpu_accelerator and self.gpu_accelerator.is_enabled():
            logger.info("Light detector initialized with GPU acceleration")
        else:
            logger.info("Light detector initialized with CPU processing")
        
    def preprocess_for_lights(self, frame: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Preprocess frame to enhance light detection with GPU acceleration"""
        # Convert to HSV for better light detection using GPU
        if self.gpu_accelerator and self.gpu_accelerator.is_enabled():
            hsv = self.gpu_accelerator.cvtColor_gpu(frame, cv2.COLOR_BGR2HSV)
        else:
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # Extract value channel (brightness)
        value_channel = hsv[:, :, 2]
        
        # Apply CLAHE for contrast enhancement (CPU-only operation)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        enhanced = clahe.apply(value_channel)
        
        # Create multiple masks for different light conditions using GPU
        if self.gpu_accelerator and self.gpu_accelerator.is_enabled():
            # 1. Direct bright spots
            bright_mask = self.gpu_accelerator.threshold_gpu(value_channel, self.brightness_threshold, 255, cv2.THRESH_BINARY)
            
            # 2. Saturated areas (very bright lights often saturate)
            saturated_mask = self.gpu_accelerator.threshold_gpu(value_channel, self.saturation_threshold, 255, cv2.THRESH_BINARY)
            
            # 3. Enhanced bright regions
            enhanced_mask = self.gpu_accelerator.threshold_gpu(enhanced, 200, 255, cv2.THRESH_BINARY)
            
            # Combine masks using GPU
            combined_mask = self.gpu_accelerator.bitwise_or_gpu(bright_mask, saturated_mask)
            combined_mask = self.gpu_accelerator.bitwise_or_gpu(combined_mask, enhanced_mask)
            
            # Morphological operations using GPU
            kernel = np.ones((3,3), np.uint8)
            combined_mask = self.gpu_accelerator.morphologyEx_gpu(combined_mask, cv2.MORPH_CLOSE, kernel)
            combined_mask = self.gpu_accelerator.morphologyEx_gpu(combined_mask, cv2.MORPH_OPEN, kernel)
        else:
            # CPU fallback
            bright_mask = cv2.threshold(value_channel, self.brightness_threshold, 255, cv2.THRESH_BINARY)[1]
            saturated_mask = cv2.threshold(value_channel, self.saturation_threshold, 255, cv2.THRESH_BINARY)[1]
            enhanced_mask = cv2.threshold(enhanced, 200, 255, cv2.THRESH_BINARY)[1]
            
            combined_mask = cv2.bitwise_or(bright_mask, saturated_mask)
            combined_mask = cv2.bitwise_or(combined_mask, enhanced_mask)
            
            kernel = np.ones((3,3), np.uint8)
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)
            combined_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_OPEN, kernel)
        
        return combined_mask, value_channel
        
    def detect_lights(self, frame: np.ndarray) -> List[DetectedLight]:
        """Detect all bright spots and lights in frame"""
        lights = []
        
        # Preprocess frame
        mask, value_channel = self.preprocess_for_lights(frame)
        
        # Find contours of bright regions
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            
            if self.min_area <= area <= self.max_area:
                # Get bounding box
                x, y, w, h = cv2.boundingRect(contour)
                
                # Get center point
                cx = x + w/2
                cy = y + h/2
                
                # Create mask for this specific light
                light_mask = np.zeros(mask.shape, dtype=np.uint8)
                cv2.drawContours(light_mask, [contour], -1, 255, -1)
                
                # Extract RGB values within the light region
                mean_vals = cv2.mean(frame, mask=light_mask)
                b, g, r = mean_vals[:3]
                
                # Calculate brightness and intensity
                brightness = np.mean([r, g, b])
                
                # Get peak intensity
                roi = value_channel[y:y+h, x:x+w]
                intensity = np.max(roi) if roi.size > 0 else brightness
                
                # Determine light type based on characteristics
                class_name = self.classify_light(r, g, b, area, intensity)
                
                light = DetectedLight(
                    x=cx,
                    y=cy,
                    width=w,
                    height=h,
                    confidence=min(1.0, intensity/255.0),  # Confidence based on intensity
                    class_name=class_name,
                    brightness=brightness,
                    r=int(r),
                    g=int(g),
                    b=int(b)
                )
                lights.append(light)
        
        return lights
    
    def classify_light(self, r: float, g: float, b: float, area: float, intensity: float) -> str:
        """Classify light based on color and characteristics with priority for high-intensity PAPI lights"""
        # Normalize RGB values
        total = r + g + b
        if total == 0:
            return "unknown_light"
        
        r_norm = r / total
        g_norm = g / total
        b_norm = b / total
        
        # Priority 1: Very high intensity lights (likely PAPI)
        if intensity > 220:
            return "high_intensity_light"
        
        # Priority 2: PAPI lights are usually white/red with high intensity
        if intensity > 180:
            if r_norm > 0.4:
                if g_norm < 0.3:
                    return "red_light"
                else:
                    return "white_light"
            # White lights (balanced RGB) with high intensity
            if abs(r_norm - 0.33) < 0.15 and abs(g_norm - 0.33) < 0.15:
                return "white_light"
        
        # Priority 3: Standard color-based classification for lower intensity lights
        # PAPI lights with moderate intensity
        if r_norm > 0.4 and intensity > 150:
            if g_norm < 0.3:
                return "red_light"
            else:
                return "white_light"
        
        # Green taxiway lights
        if g_norm > 0.4:
            return "green_light"
        
        # Blue taxiway edge lights
        if b_norm > 0.4:
            return "blue_light"
        
        # Yellow/amber lights
        if r_norm > 0.35 and g_norm > 0.35 and b_norm < 0.3:
            return "yellow_light"
        
        # White lights (balanced RGB)
        if abs(r_norm - 0.33) < 0.1 and abs(g_norm - 0.33) < 0.1:
            return "white_light"
        
        return "runway_light"


class VideoProcessor:
    """Process drone videos for PAPI light measurements"""
    
    @staticmethod
    def extract_first_frame(video_path: str, output_path: str) -> Dict:
        """Extract first frame from video and get metadata"""
        try:
            cap = cv2.VideoCapture(video_path)
            ret, frame = cap.read()
            
            if ret:
                cv2.imwrite(output_path, frame)
                
                # Extract metadata (this would come from drone telemetry)
                metadata = {
                    "frame_width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                    "frame_height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                    "fps": cap.get(cv2.CAP_PROP_FPS),
                    "total_frames": int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                }
                
                cap.release()
                return metadata
            
            cap.release()
            return {}
            
        except Exception as e:
            logger.error(f"Error extracting first frame: {e}")
            return {}
    
    @staticmethod
    def detect_lights(image_path: str, reference_points: List[Dict]) -> Dict[str, Dict]:
        """Detect PAPI lights in image using advanced computer vision with line detection"""
        try:
            img = cv2.imread(image_path)
            if img is None:
                logger.error(f"Could not load image: {image_path}")
                return {}
            
            height, width = img.shape[:2]
            detector = RunwayLightDetector()
            
            # Detect all lights in the image
            detected_lights_list = detector.detect_lights(img)
            
            if not detected_lights_list:
                logger.warning("No lights detected, using default positions")
                return VideoProcessor._generate_default_positions(width, height)
            
            # Enhanced PAPI detection with line-based approach
            papi_candidates = VideoProcessor._filter_papi_candidates(detected_lights_list)
            
            if not papi_candidates:
                logger.warning("No high-intensity PAPI candidates found, using default positions")
                return VideoProcessor._generate_default_positions(width, height)
            
            # Find the best line of 4 PAPI lights
            best_papi_line = VideoProcessor._find_best_papi_line(papi_candidates)
            
            if best_papi_line:
                logger.info(f"Found PAPI line with {len(best_papi_line)} lights, avg intensity: {np.mean([l.intensity for l in best_papi_line]):.1f}")
                return VideoProcessor._convert_to_papi_positions(best_papi_line, width, height)
            else:
                logger.warning("No coherent PAPI light line found, using fallback method")
                return VideoProcessor._fallback_papi_detection(papi_candidates, width, height)
                
        except Exception as e:
            logger.error(f"Error in light detection: {e}")
            return VideoProcessor._generate_default_positions(width, height)
    
    @staticmethod
    def _filter_papi_candidates(detected_lights_list: List[DetectedLight]) -> List[DetectedLight]:
        """Filter lights that could be PAPI lights based on intensity, size, and characteristics"""
        papi_candidates = []
        
        # Calculate area statistics for size-based filtering
        areas = [max(light.width * light.height, light.width * light.width, light.height * light.height) for light in detected_lights_list]
        avg_area = np.mean(areas) if areas else 0
        
        for light in detected_lights_list:
            light_area = max(light.width * light.height, light.width * light.width, light.height * light.height)
            
            # Primary filter: High intensity PAPI lights (very bright)
            if light.intensity > 200:
                papi_candidates.append(light)
                continue
            
            # Secondary filter: Large lights with good intensity (PAPI lights are typically bigger)
            if light_area > avg_area * 1.2 and light.intensity > 150:
                papi_candidates.append(light)
                continue
            
            # Tertiary filter: High brightness with specific light types
            if (light.brightness > 180 and 
                light.class_name in ["white_light", "red_light", "high_intensity_light"]):
                papi_candidates.append(light)
                continue
            
            # Quaternary filter: Large lights with high brightness (surface area priority)
            if light_area > avg_area * 1.5 and light.brightness > 160:
                papi_candidates.append(light)
                continue
            
            # Final filter: Very bright lights regardless of size
            if light.brightness > 220:
                papi_candidates.append(light)
        
        logger.info(f"Found {len(papi_candidates)} potential PAPI candidates after intensity and size filtering")
        return papi_candidates
    
    @staticmethod
    def _find_best_papi_line(candidates: List[DetectedLight]) -> List[DetectedLight]:
        """Find the best line of 4 PAPI lights using geometric and intensity analysis"""
        if len(candidates) < 4:
            return []
        
        best_line = []
        best_score = -1
        
        # Try different combinations of 4 lights
        from itertools import combinations
        
        for combo in combinations(candidates, 4):
            score = VideoProcessor._score_papi_line(list(combo))
            if score > best_score:
                best_score = score
                best_line = list(combo)
        
        # Require minimum score threshold for valid PAPI line
        if best_score > 0.5:
            return sorted(best_line, key=lambda x: x.x)  # Sort left to right
        
        return []
    
    @staticmethod
    def _score_papi_line(lights: List[DetectedLight]) -> float:
        """Score a potential PAPI light line based on geometric alignment, intensity, and size"""
        if len(lights) != 4:
            return 0.0
        
        # Sort by x-coordinate
        sorted_lights = sorted(lights, key=lambda x: x.x)
        
        # Check horizontal alignment (Y-coordinates should be similar)
        y_coords = [light.y for light in sorted_lights]
        y_std = np.std(y_coords)
        alignment_score = max(0, 1 - (y_std / 50))  # Penalty for vertical misalignment
        
        # Check spacing consistency (lights should be evenly spaced)
        x_coords = [light.x for light in sorted_lights]
        spacings = [x_coords[i+1] - x_coords[i] for i in range(3)]
        spacing_std = np.std(spacings)
        spacing_score = max(0, 1 - (spacing_std / 30))  # Penalty for uneven spacing
        
        # Check intensity consistency (PAPI lights should have similar high intensity)
        intensities = [light.intensity for light in sorted_lights]
        avg_intensity = np.mean(intensities)
        intensity_score = min(1.0, avg_intensity / 250)  # Reward high intensity
        
        # Check size consistency and overall size (PAPI lights should be substantial and similar)
        areas = [max(light.width * light.height, light.width * light.width, light.height * light.height) 
                for light in sorted_lights]
        avg_area = np.mean(areas)
        area_std = np.std(areas)
        
        # Size score: reward larger lights and penalize size inconsistency
        size_score = min(1.0, avg_area / 100)  # Reward larger average area
        size_consistency_score = max(0, 1 - (area_std / avg_area if avg_area > 0 else 1))  # Penalize size variation
        combined_size_score = (size_score * 0.6 + size_consistency_score * 0.4)
        
        # Check that lights form a reasonable line length
        line_length = x_coords[-1] - x_coords[0]
        length_score = 1.0 if 100 < line_length < 400 else 0.5  # Reasonable PAPI spacing
        
        # Combined score with size consideration
        total_score = (alignment_score * 0.25 + 
                      spacing_score * 0.2 + 
                      intensity_score * 0.3 + 
                      combined_size_score * 0.15 +  # New size component
                      length_score * 0.1)
        
        logger.debug(f"Line score: {total_score:.3f} (align:{alignment_score:.2f}, space:{spacing_score:.2f}, "
                    f"intensity:{intensity_score:.2f}, size:{combined_size_score:.2f}, length:{length_score:.2f})")
        
        return total_score
    
    @staticmethod
    def _convert_to_papi_positions(lights: List[DetectedLight], width: int, height: int) -> Dict[str, Dict]:
        """Convert detected lights to PAPI position format"""
        detected_lights = {}
        papi_names = ["PAPI_A", "PAPI_B", "PAPI_C", "PAPI_D"]
        
        for i, light in enumerate(lights):
            if i < len(papi_names):
                x_percent = (light.x / width) * 100
                y_percent = (light.y / height) * 100
                size_percent = max(8, min(15, (max(light.width, light.height) / width) * 100))
                
                light_area = max(light.width * light.height, light.width * light.width, light.height * light.height)
                logger.info(f"Assigning {papi_names[i]}: pos=({x_percent:.1f}%, {y_percent:.1f}%), "
                          f"intensity={light.intensity:.1f}, brightness={light.brightness:.1f}, "
                          f"area={light_area:.1f}pxÂ²")
                
                detected_lights[papi_names[i]] = {
                    "x": x_percent,
                    "y": y_percent,
                    "size": size_percent,
                    "width": (light.width / width) * 100,
                    "height": (light.height / height) * 100,
                    "confidence": light.confidence,
                    "class_name": light.class_name,
                    "brightness": light.brightness,
                    "intensity": light.intensity
                }
        
        return detected_lights
    
    @staticmethod
    def _fallback_papi_detection(candidates: List[DetectedLight], width: int, height: int) -> Dict[str, Dict]:
        """Fallback PAPI detection using combined intensity and size scoring"""
        # Calculate combined score for each candidate (intensity + size)
        for candidate in candidates:
            light_area = max(candidate.width * candidate.height, 
                           candidate.width * candidate.width, 
                           candidate.height * candidate.height)
            
            # Normalize scores (intensity out of 255, area as relative score)
            intensity_score = candidate.intensity / 255.0
            size_score = min(1.0, light_area / 200)  # Normalize area to 0-1 scale
            
            # Combined score: 60% intensity, 40% size (PAPI lights should be both bright and large)
            candidate.combined_score = (intensity_score * 0.6) + (size_score * 0.4)
        
        # Sort by combined score (highest first)
        candidates.sort(key=lambda x: x.combined_score, reverse=True)
        
        # Take top 4 candidates with best combined score
        top_candidates = candidates[:4]
        
        logger.info(f"Fallback detection selected 4 lights with combined scores: "
                   f"{[f'{c.combined_score:.2f}' for c in top_candidates]}")
        
        # Sort by x-position for proper PAPI ordering (A->B->C->D from left to right)
        top_candidates.sort(key=lambda x: x.x)
        
        return VideoProcessor._convert_to_papi_positions(top_candidates, width, height)
    
    @staticmethod
    def _generate_default_positions(width: int, height: int) -> Dict[str, Dict]:
        """Generate default PAPI positions when detection fails"""
        detected_lights = {}
        base_x = width // 3
        base_y = height // 2
        
        for i, light_type in enumerate(["PAPI_A", "PAPI_B", "PAPI_C", "PAPI_D"]):
            x_percent = ((base_x + (i * 100)) / width) * 100
            y_percent = (base_y / height) * 100
            
            detected_lights[light_type] = {
                "x": x_percent,
                "y": y_percent,
                "size": 8
            }
        
        return detected_lights
    
    @staticmethod
    def process_frame(frame: np.ndarray, light_positions: Dict, 
                     drone_data: Dict, reference_points: Dict = None) -> Dict:
        """Process single frame for light measurements using actual tracked positions"""
        measurements = {}
        
        # Debug logging
        logger.info(f"Processing frame with light_positions: {light_positions}")
        
        height, width = frame.shape[:2]
        
        for light_name, pos in light_positions.items():
            if light_name not in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                continue
                
            # Determine pixel coordinates
            if 'x' in pos and 'y' in pos and pos['x'] > 100 and pos['y'] > 100:
                # Use tracked position directly (already in pixel coordinates)
                pixel_x = int(pos['x'])
                pixel_y = int(pos['y'])
                pixel_size = int(pos.get('size', 20))
                # Check if RGB values are provided from tracking
                rgb_values = pos.get('rgb', None)
            else:
                # Treat as percentage coordinates (most common case)
                x, y = pos.get("x", 50), pos.get("y", 50)
                size = pos.get("size", pos.get("width", 8))
                
                pixel_x = int((x / 100) * width)
                pixel_y = int((y / 100) * height)
                # Use fixed pixel size for ROI extraction instead of percentage-based
                # PAPI lights are small, so 20-30 pixels radius is sufficient
                pixel_size = 40  # Fixed 40px ROI (20px radius)
                rgb_values = None  # Force RGB extraction
            
            # Extract RGB from frame if not provided by tracking
            if rgb_values is None:
                half_size = pixel_size // 2
                x1 = max(0, pixel_x - half_size)
                y1 = max(0, pixel_y - half_size)
                x2 = min(width, pixel_x + half_size)
                y2 = min(height, pixel_y + half_size)
                
                roi = frame[y1:y2, x1:x2]
                if roi.size > 0:
                    mean_rgb = cv2.mean(roi)[:3]
                    rgb_values = [int(mean_rgb[2]), int(mean_rgb[1]), int(mean_rgb[0])]  # BGR to RGB
                else:
                    rgb_values = [255, 255, 255]
            
            # Use RGB values from tracking if available
            if isinstance(rgb_values, list) and len(rgb_values) >= 3:
                r, g, b = rgb_values[0], rgb_values[1], rgb_values[2]
            else:
                r, g, b = 255, 255, 255
            
            # Calculate intensity
            intensity = np.mean([r, g, b])
            
            # Determine light status using enhanced classification
            status = classify_light_status(r, g, b, intensity)
            
            # Get reference point GPS coordinates for this PAPI light
            papi_gps = None
            if "ref_points" in drone_data and light_name in drone_data["ref_points"]:
                papi_gps = drone_data["ref_points"][light_name]
            else:
                # Fallback GPS coordinates if not available
                papi_gps = {"latitude": 48.85959167, "longitude": 17.98482778, "elevation": 245.592}
            
            # Calculate angles and distances using GPS coordinates
            angle = calculate_angle(drone_data, papi_gps)
            distance_ground = calculate_ground_distance(drone_data, papi_gps)
            distance_direct = calculate_direct_distance(drone_data, papi_gps)
            
            measurements[light_name] = {
                "status": status,
                "rgb": {"r": r, "g": g, "b": b},
                "intensity": intensity,
                "angle": angle,
                "distance_ground": distance_ground,
                "distance_direct": distance_direct
            }
        
        return measurements
    
    @staticmethod
    def create_light_video(video_path: str, light_position: Dict, 
                          output_path: str):
        """Create cropped video for single PAPI light"""
        cap = cv2.VideoCapture(video_path)
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # Handle both old format (width/height) and new format (size)
        if "width" in light_position and "height" in light_position:
            x, y, w, h = light_position["x"], light_position["y"], \
                        light_position["width"], light_position["height"]
        else:
            # Convert from percentage-based position and size to pixel coordinates
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # Convert percentage to pixels
            x_pct = light_position["x"]
            y_pct = light_position["y"] 
            size_pct = light_position["size"]
            
            x = int((x_pct / 100) * frame_width)
            y = int((y_pct / 100) * frame_height)
            w = h = int((size_pct / 100) * frame_width)  # Square region
        
        # Create video writer with H.264 codec for browser compatibility
        fourcc = cv2.VideoWriter_fourcc(*'avc1')  # H.264 codec
        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Extract and write ROI
            roi = frame[y:y+h, x:x+w]
            out.write(roi)
        
        cap.release()
        out.release()


def calculate_angle(drone_data: Dict, light_pos: Dict) -> float:
    """Calculate angle between drone and light using GPS coordinates"""
    try:
        # Get drone position
        drone_lat = drone_data.get("latitude")
        drone_lon = drone_data.get("longitude") 
        drone_alt = drone_data.get("elevation", drone_data.get("altitude", 100))
        
        # Get PAPI light position (from reference points)
        papi_lat = light_pos.get("latitude")
        papi_lon = light_pos.get("longitude")
        papi_elevation = light_pos.get("elevation", 0)
        
        # Debug logging
        logger.debug(f"Angle calculation - Drone: lat={drone_lat}, lon={drone_lon}, alt={drone_alt}")
        logger.debug(f"Angle calculation - PAPI: lat={papi_lat}, lon={papi_lon}, elev={papi_elevation}")
        
        if None in [drone_lat, drone_lon, papi_lat, papi_lon]:
            logger.error(f"Missing GPS data - cannot calculate accurate PAPI angle without coordinates")
            raise ValueError("GPS coordinates are required for accurate PAPI angle calculation")
        
        # Calculate ground distance using Haversine formula
        ground_dist = haversine_distance(drone_lat, drone_lon, papi_lat, papi_lon)
        
        # Calculate height difference (drone elevation minus PAPI elevation)
        # This gives positive angles when drone is above PAPI (normal case for glide slope)
        height_diff = drone_alt - papi_elevation
        
        # Debug logging
        logger.debug(f"Angle calculation - Ground distance: {ground_dist:.1f}m, Height diff: {height_diff:.1f}m")
        
        # Calculate angle (elevation angle from horizontal)
        if ground_dist > 0:
            angle = np.degrees(np.arctan(height_diff / ground_dist))
        else:
            angle = 90.0 if height_diff > 0 else -90.0
        
        logger.debug(f"Angle calculation - Final angle: {angle:.6f}Â°")
        return angle
        
    except Exception as e:
        logger.warning(f"Error calculating angle: {e}")
        return 0.0


def calculate_ground_distance(drone_data: Dict, light_pos: Dict) -> float:
    """Calculate ground distance between drone and light using GPS"""
    try:
        drone_lat = drone_data.get("latitude")
        drone_lon = drone_data.get("longitude")
        papi_lat = light_pos.get("latitude")
        papi_lon = light_pos.get("longitude")
        
        if None in [drone_lat, drone_lon, papi_lat, papi_lon]:
            return 500.0  # Fallback distance
            
        return haversine_distance(drone_lat, drone_lon, papi_lat, papi_lon)
        
    except Exception as e:
        logger.warning(f"Error calculating ground distance: {e}")
        return 500.0


def calculate_direct_distance(drone_data: Dict, light_pos: Dict) -> float:
    """Calculate direct 3D distance between drone and light"""
    try:
        ground_dist = calculate_ground_distance(drone_data, light_pos)
        
        drone_alt = drone_data.get("elevation", drone_data.get("altitude", 100))
        papi_elevation = light_pos.get("elevation", 0)
        height_diff = drone_alt - papi_elevation
        
        return math.sqrt(ground_dist**2 + height_diff**2)
        
    except Exception as e:
        logger.warning(f"Error calculating direct distance: {e}")
        return 500.0


def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Calculate the great circle distance between two points on Earth (in meters)"""
    R = 6371000  # Earth's radius in meters
    
    phi1 = math.radians(lat1)
    phi2 = math.radians(lat2)
    delta_phi = math.radians(lat2 - lat1)
    delta_lambda = math.radians(lon2 - lon1)
    
    a = (math.sin(delta_phi / 2) * math.sin(delta_phi / 2) +
         math.cos(phi1) * math.cos(phi2) *
         math.sin(delta_lambda / 2) * math.sin(delta_lambda / 2))
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    
    return R * c


def classify_light_status(r: float, g: float, b: float, intensity: float) -> str:
    """Classify light status as per PAPI requirements"""
    # Improved classification logic based on RGB values
    if intensity < 30:  # Very dark
        return "not_visible"
    
    # Normalize RGB values
    total = r + g + b
    if total == 0:
        return "not_visible"
    
    r_norm = r / total
    g_norm = g / total
    b_norm = b / total
    
    # Red light detection (PAPI red sector)
    if r_norm > 0.45 and g_norm < 0.35 and b_norm < 0.35:
        return "red"
    
    # White light detection (PAPI white sector)  
    if (abs(r_norm - 0.33) < 0.15 and 
        abs(g_norm - 0.33) < 0.15 and 
        abs(b_norm - 0.33) < 0.15 and
        intensity > 80):
        return "white"
    
    # Transition zone (color changing)
    if (0.35 < r_norm < 0.5 and 0.25 < g_norm < 0.4):
        return "transition"
    
    # Default to not_visible for unclear cases
    return "not_visible"


@dataclass
class TrackedPAPILight:
    """Represents a PAPI light tracked over multiple frames"""
    light_name: str  # PAPI_A, PAPI_B, PAPI_C, PAPI_D
    positions: List[Tuple[int, int]]  # (x, y) positions over time
    rgb_values: List[Tuple[int, int, int]]  # RGB values over time
    frame_numbers: List[int]  # Frame numbers where detected
    confidence_scores: List[float]  # Detection confidence scores
    sizes: List[int]  # Light sizes over time
    
    def get_last_position(self) -> Tuple[int, int]:
        """Get most recent position"""
        return self.positions[-1] if self.positions else (0, 0)
    
    def get_velocity(self, frame_gap: int = 1) -> Tuple[float, float]:
        """Calculate velocity based on recent positions"""
        if len(self.positions) < 2:
            return 0.0, 0.0
        
        # Use last few positions for velocity calculation
        recent_positions = self.positions[-min(5, len(self.positions)):]
        recent_frames = self.frame_numbers[-len(recent_positions):]
        
        if len(recent_positions) >= 2:
            dt = recent_frames[-1] - recent_frames[0]
            if dt > 0:
                vx = (recent_positions[-1][0] - recent_positions[0][0]) / dt
                vy = (recent_positions[-1][1] - recent_positions[0][1]) / dt
                return vx, vy
        
        return 0.0, 0.0
    
    def predict_position(self, frame_gap: int) -> Tuple[int, int]:
        """Predict position based on motion history"""
        if not self.positions:
            return 0, 0
        
        last_x, last_y = self.get_last_position()
        vx, vy = self.get_velocity()
        
        pred_x = int(last_x + vx * frame_gap)
        pred_y = int(last_y + vy * frame_gap)
        
        return pred_x, pred_y


class PAPILightTracker:
    """Advanced PAPI light tracker with motion prediction based on prototype algorithm"""
    
    def __init__(self, initial_positions: Dict, frame_width: int, frame_height: int):
        self.frame_width = frame_width
        self.frame_height = frame_height
        self.light_detector = RunwayLightDetector()
        self.max_distance = 50  # Maximum matching distance
        self.max_frame_gap = 20  # Maximum frames without detection
        
        # Initialize tracked lights from manual positions
        self.tracked_lights: Dict[str, TrackedPAPILight] = {}
        valid_lights_count = 0
        
        for light_name, pos in initial_positions.items():
            if light_name in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                # Handle different position data formats
                if isinstance(pos, dict):
                    if 'x' in pos and 'y' in pos:
                        # Use provided coordinates
                        pixel_x = int((pos['x'] / 100) * frame_width)
                        pixel_y = int((pos['y'] / 100) * frame_height)
                        pixel_size = int((pos.get('size', 8) / 100) * frame_width)
                        valid_lights_count += 1
                    else:
                        # Use fallback default positions if coordinates are missing
                        logger.warning(f"Using fallback position for {light_name}: incomplete data: {pos}")
                        light_index = ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D'].index(light_name)
                        pixel_x = int((20 + light_index * 20) / 100 * frame_width)
                        pixel_y = int(50 / 100 * frame_height)
                        pixel_size = int(8 / 100 * frame_width)
                        valid_lights_count += 1
                else:
                    # Handle completely invalid position data
                    logger.warning(f"Using fallback position for {light_name}: invalid data: {pos}")
                    light_index = ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D'].index(light_name)
                    pixel_x = int((20 + light_index * 20) / 100 * frame_width)
                    pixel_y = int(50 / 100 * frame_height)
                    pixel_size = int(8 / 100 * frame_width)
                    valid_lights_count += 1
                    
                self.tracked_lights[light_name] = TrackedPAPILight(
                    light_name=light_name,
                    positions=[(pixel_x, pixel_y)],
                    rgb_values=[(255, 255, 255)],  # Default white
                    frame_numbers=[0],
                    confidence_scores=[0.0],
                    sizes=[pixel_size]
                )
        
        if valid_lights_count == 0:
            logger.error("No valid PAPI lights could be initialized for tracking")
        else:
            logger.info(f"Initialized {valid_lights_count} PAPI lights for tracking")
        
        self.global_motion = (0.0, 0.0)  # Estimated camera motion
        self.prev_detections = []  # Previous frame detections for motion estimation
    
    def estimate_global_motion(self, current_lights: List, prev_lights: List) -> Tuple[float, float]:
        """Estimate global camera motion between frames using detected lights"""
        if len(current_lights) < 3 or len(prev_lights) < 3:
            return 0.0, 0.0
        
        motions = []
        for curr_light in current_lights:
            best_dist = float('inf')
            best_prev = None
            
            for prev_light in prev_lights:
                # Only match lights of similar brightness and characteristics
                if abs(curr_light.brightness - prev_light.brightness) > 50:
                    continue
                
                dist = math.sqrt((curr_light.x - prev_light.x)**2 + (curr_light.y - prev_light.y)**2)
                if dist < 100 and dist < best_dist:  # Reasonable movement threshold
                    best_dist = dist
                    best_prev = prev_light
            
            if best_prev:
                dx = curr_light.x - best_prev.x
                dy = curr_light.y - best_prev.y
                motions.append((dx, dy))
        
        if len(motions) < 3:
            return 0.0, 0.0
        
        # Use median to get robust motion estimate (removes outliers)
        motions = np.array(motions)
        median_dx = np.median(motions[:, 0])
        median_dy = np.median(motions[:, 1])
        
        return median_dx, median_dy
    
    def update_frame(self, frame: np.ndarray, frame_number: int) -> Dict:
        """Update light positions for current frame using sophisticated tracking"""
        # Detect all lights in current frame
        detected_lights = self.light_detector.detect_lights(frame)
        
        # Estimate global motion if we have previous detections
        if self.prev_detections and len(self.prev_detections) >= 3 and len(detected_lights) >= 3:
            self.global_motion = self.estimate_global_motion(detected_lights, self.prev_detections)
        
        # Update each tracked PAPI light
        frame_positions = {}
        unmatched_detections = list(detected_lights)
        
        for light_name, tracked_light in self.tracked_lights.items():
            last_frame = tracked_light.frame_numbers[-1] if tracked_light.frame_numbers else 0
            frame_gap = frame_number - last_frame
            
            # Skip if track is too old
            if frame_gap > self.max_frame_gap:
                # Use global motion to predict position
                last_x, last_y = tracked_light.get_last_position()
                pred_x = int(last_x + self.global_motion[0] * frame_gap)
                pred_y = int(last_y + self.global_motion[1] * frame_gap)
                
                frame_positions[light_name] = {
                    'x': pred_x,
                    'y': pred_y,
                    'size': tracked_light.sizes[-1] if tracked_light.sizes else 20,
                    'rgb': tracked_light.rgb_values[-1] if tracked_light.rgb_values else [255, 255, 255],
                    'confidence': 0.1  # Low confidence for predicted position
                }
                continue
            
            # Predict where this light should be
            pred_x, pred_y = tracked_light.predict_position(frame_gap)
            
            # Apply global motion correction
            pred_x += int(self.global_motion[0] * frame_gap)
            pred_y += int(self.global_motion[1] * frame_gap)
            
            # Find best matching detection
            best_match = None
            best_score = float('inf')
            best_match_idx = -1
            
            for idx, detection in enumerate(unmatched_detections):
                # Distance to predicted position
                pred_distance = math.sqrt((detection.x - pred_x)**2 + (detection.y - pred_y)**2)
                
                # Distance to last known position
                last_x, last_y = tracked_light.get_last_position()
                last_distance = math.sqrt((detection.x - last_x)**2 + (detection.y - last_y)**2)
                
                # Combined score (weighted towards prediction)
                score = 0.7 * pred_distance + 0.3 * last_distance
                
                # Penalty for significant brightness change
                if tracked_light.rgb_values:
                    last_rgb = tracked_light.rgb_values[-1]
                    last_brightness = sum(last_rgb) / 3
                    brightness_diff = abs(detection.brightness - last_brightness)
                    score += brightness_diff * 0.05
                
                # Penalty for unreasonable movement
                if frame_gap > 0:
                    movement_per_frame = last_distance / frame_gap
                    if movement_per_frame > 30:  # More than 30 pixels per frame is suspicious
                        score += movement_per_frame * 0.5
                
                if score < self.max_distance and score < best_score:
                    best_score = score
                    best_match = detection
                    best_match_idx = idx
            
            if best_match:
                # Update tracked light with detection
                tracked_light.positions.append((int(best_match.x), int(best_match.y)))
                tracked_light.rgb_values.append((best_match.r, best_match.g, best_match.b))
                tracked_light.frame_numbers.append(frame_number)
                tracked_light.confidence_scores.append(best_match.confidence)
                tracked_light.sizes.append(max(best_match.width, best_match.height))
                
                # Remove from unmatched list
                if best_match_idx >= 0:
                    unmatched_detections.pop(best_match_idx)
                
                frame_positions[light_name] = {
                    'x': int(best_match.x),
                    'y': int(best_match.y),
                    'size': max(best_match.width, best_match.height),
                    'rgb': [best_match.r, best_match.g, best_match.b],
                    'confidence': best_match.confidence
                }
            else:
                # No detection found, use prediction
                last_rgb = tracked_light.rgb_values[-1] if tracked_light.rgb_values else [255, 255, 255]
                last_size = tracked_light.sizes[-1] if tracked_light.sizes else 20
                
                # Optionally update with interpolated position
                if frame_gap <= 5:  # Only interpolate for short gaps
                    tracked_light.positions.append((pred_x, pred_y))
                    tracked_light.rgb_values.append(last_rgb)
                    tracked_light.frame_numbers.append(frame_number)
                    tracked_light.confidence_scores.append(0.2)  # Low confidence
                    tracked_light.sizes.append(last_size)
                
                frame_positions[light_name] = {
                    'x': pred_x,
                    'y': pred_y,
                    'size': last_size,
                    'rgb': last_rgb,
                    'confidence': 0.2
                }
        
        # Store current detections for next iteration
        self.prev_detections = detected_lights
        
        return frame_positions


class PAPIVideoGenerator:
    """Generate individual PAPI light videos from the main video with GPU acceleration"""
    
    def __init__(self, output_dir: str, use_gpu: bool = True, batch_size: int = 8):
        self.output_dir = output_dir
        self.use_gpu = use_gpu
        self.batch_size = batch_size
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize GPU acceleration and batch processor
        self.gpu_accelerator = GPUAccelerator() if use_gpu else None
        self.batch_processor = BatchFrameProcessor(batch_size=batch_size)
        self.frame_cache = FrameProcessingCache()
        
        if self.gpu_accelerator and self.gpu_accelerator.is_enabled():
            logger.info(f"Video generator initialized with GPU acceleration (batch size: {batch_size})")
        else:
            logger.info(f"Video generator initialized with CPU processing (batch size: {batch_size})")
    
    def generate_enhanced_main_video(self, video_path: str, session_id: str, 
                                   light_positions: Dict, measurements_data: List[Dict],
                                   drone_telemetry: List[Dict] = None, 
                                   reference_points: Dict = None) -> str:
        """Generate enhanced version of main video with real drone GPS data overlay and tracked PAPI light rectangles"""
        try:
            # Extract real GPS data from video file
            gps_extractor = GPSExtractor()
            real_gps_data = gps_extractor.extract_gps_data(video_path)
            if real_gps_data:
                logger.info(f"Successfully extracted {len(real_gps_data)} GPS data points from video")
            else:
                logger.warning("No GPS data found in video - will use fallback coordinates")
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"Failed to open video: {video_path}")
                return ""
            
            # Get video properties
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if total_frames <= 0:
                logger.error(f"Invalid video: zero frames in {video_path}")
                cap.release()
                return ""
            
            # Initialize PAPI light tracker
            light_tracker = PAPILightTracker(light_positions, frame_width, frame_height)
            
            # Check if any lights were successfully initialized
            if not light_tracker.tracked_lights:
                logger.error("No PAPI lights initialized - cannot generate enhanced video")
                cap.release()
                return ""
            
            # Create output video path
            enhanced_video_filename = f"{session_id}_enhanced_main_video.mp4"
            enhanced_video_path = os.path.join(self.output_dir, enhanced_video_filename)
            
            # Create video writer with H.264 codec for browser compatibility
            fourcc = cv2.VideoWriter_fourcc(*'avc1')  # H.264 codec
            out = cv2.VideoWriter(enhanced_video_path, fourcc, fps, (frame_width, frame_height))
            
            if not out.isOpened():
                logger.error(f"Failed to initialize video writer for {enhanced_video_path}")
                cap.release()
                return ""
            
            frame_count = 0
            frames_written = 0
            
            # Pre-compute GPS data for all frames to avoid repeated interpolation
            gps_cache = {}
            if real_gps_data:
                logger.info("Pre-computing GPS data for all frames...")
                start_time = time.time()
                gps_extractor = GPSExtractor()
                
                # Batch compute GPS data every 10 frames to reduce computation
                for i in range(0, total_frames, 10):
                    end_frame = min(i + 10, total_frames)
                    for frame_idx in range(i, end_frame):
                        interpolated_gps = gps_extractor.interpolate_gps_for_frame(real_gps_data, frame_idx, fps)
                        if interpolated_gps:
                            gps_cache[frame_idx] = {
                                "latitude": interpolated_gps.latitude,
                                "longitude": interpolated_gps.longitude, 
                                "elevation": interpolated_gps.altitude,
                                "speed": interpolated_gps.speed or 0.0,
                                "heading": interpolated_gps.heading or 0.0,
                                "satellites": interpolated_gps.satellites,
                                "accuracy": interpolated_gps.accuracy
                            }
                
                gps_time = time.time() - start_time
                logger.info(f"GPS pre-computation completed in {gps_time:.2f}s for {len(gps_cache)} frames")
            
            # Process frames in batches for better GPU utilization
            frame_batch = []
            frame_indices = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    # Process remaining frames in batch
                    if frame_batch:
                        self._process_frame_batch(frame_batch, frame_indices, light_tracker, out, 
                                                total_frames, measurements_data, drone_telemetry, 
                                                reference_points, gps_cache, fps)
                        frames_written += len(frame_batch)
                    break
                
                frame_batch.append(frame)
                frame_indices.append(frame_count)
                frame_count += 1
                
                # Process batch when full or at end of video
                if len(frame_batch) >= self.batch_size:
                    try:
                        self._process_frame_batch(frame_batch, frame_indices, light_tracker, out, 
                                                total_frames, measurements_data, drone_telemetry, 
                                                reference_points, gps_cache, fps)
                        frames_written += len(frame_batch)
                        
                        # Progress logging
                        if frame_count % 100 <= self.batch_size:
                            elapsed = time.time() - start_time if 'start_time' in locals() else 0
                            fps_rate = frame_count / elapsed if elapsed > 0 else 0
                            logger.info(f"Enhanced video: processed {frame_count}/{total_frames} frames ({fps_rate:.1f} fps)")
                            
                    except Exception as batch_error:
                        logger.warning(f"Error processing frame batch at {frame_indices[0]}: {batch_error}")
                        # Write original frames without enhancements
                        for original_frame in frame_batch:
                            out.write(original_frame)
                        frames_written += len(frame_batch)
                    
                    # Reset batch
                    frame_batch = []
                    frame_indices = []
            
            # Release resources
            cap.release()
            out.release()
            
            # Verify that we actually wrote frames
            if frames_written == 0:
                logger.error(f"No frames written to enhanced video - removing empty file")
                if os.path.exists(enhanced_video_path):
                    os.remove(enhanced_video_path)
                return ""
            
            # Verify file size
            if os.path.exists(enhanced_video_path):
                file_size = os.path.getsize(enhanced_video_path)
                if file_size < 1000:  # Less than 1KB is likely invalid
                    logger.error(f"Enhanced video file too small ({file_size} bytes) - removing")
                    os.remove(enhanced_video_path)
                    return ""
                logger.info(f"Enhanced main video generated: {enhanced_video_path} ({file_size} bytes, {frames_written} frames)")
            else:
                logger.error(f"Enhanced video file was not created: {enhanced_video_path}")
                return ""
                
            return enhanced_video_path
            
        except Exception as e:
            logger.error(f"Error generating enhanced main video: {e}")
            # Clean up any partial file
            enhanced_video_filename = f"{session_id}_enhanced_main_video.mp4"
            enhanced_video_path = os.path.join(self.output_dir, enhanced_video_filename)
            if os.path.exists(enhanced_video_path):
                os.remove(enhanced_video_path)
            return ""
    
    def _process_frame_batch(self, frame_batch: List[np.ndarray], frame_indices: List[int], 
                            light_tracker, out, total_frames: int, measurements_data: List[Dict],
                            drone_telemetry: List[Dict], reference_points: Dict, 
                            gps_cache: Dict, fps: float):
        """Process a batch of frames for GPU optimization"""
        enhanced_frames = []
        
        for i, (frame, frame_idx) in enumerate(zip(frame_batch, frame_indices)):
            try:
                # Update light positions for current frame
                tracked_positions = light_tracker.update_frame(frame, frame_idx)
                
                # Get pre-computed GPS data
                drone_data = gps_cache.get(frame_idx)
                
                # Enhance frame with overlays using tracked positions and cached GPS data
                enhanced_frame = self._add_overlays_to_frame_with_tracking_optimized(
                    frame, tracked_positions, frame_idx, total_frames, 
                    measurements_data, drone_telemetry, reference_points, drone_data
                )
                
                enhanced_frames.append(enhanced_frame)
                
            except Exception as frame_error:
                logger.warning(f"Error processing frame {frame_idx}: {frame_error}")
                enhanced_frames.append(frame)  # Use original frame
        
        # Write all enhanced frames
        for enhanced_frame in enhanced_frames:
            out.write(enhanced_frame)
    
    def _add_overlays_to_frame_with_tracking_optimized(self, frame: np.ndarray, tracked_positions: Dict, 
                                                      frame_number: int, total_frames: int,
                                                      measurements_data: List[Dict] = None,
                                                      drone_telemetry: List[Dict] = None,
                                                      reference_points: Dict = None,
                                                      cached_drone_data: Dict = None) -> np.ndarray:
        """Optimized version with pre-computed GPS data"""
        enhanced_frame = frame.copy()
        height, width = frame.shape[:2]
        
        # Draw tracked PAPI light rectangles with current RGB values
        for light_name, pos in tracked_positions.items():
            if light_name not in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                continue
            
            if not isinstance(pos, dict) or 'x' not in pos or 'y' not in pos:
                continue
                
            pixel_x = pos['x']
            pixel_y = pos['y']
            pixel_size = pos.get('size', 20)
            confidence = pos.get('confidence', 0.0)
            
            # Calculate rectangle bounds
            half_size = pixel_size // 2
            x1 = max(0, pixel_x - half_size)
            y1 = max(0, pixel_y - half_size)
            x2 = min(width, pixel_x + half_size)
            y2 = min(height, pixel_y + half_size)
            
            # Get RGB values from tracking if available
            rgb_values = pos.get('rgb', [255, 255, 255])
            if len(rgb_values) >= 3:
                r, g, b = rgb_values[0], rgb_values[1], rgb_values[2]
            else:
                r, g, b = 255, 255, 255
            
            # Determine rectangle color based on light status
            status = classify_light_status(r, g, b, np.mean([r, g, b]))
            color_map = {
                'red_light': (0, 0, 255),
                'white_light': (255, 255, 255), 
                'green_light': (0, 255, 0),
                'blue_light': (255, 0, 0),
                'transition': (0, 165, 255),
                'not_visible': (128, 128, 128),
            }
            rect_color = color_map.get(status, (0, 255, 255))
            
            # Draw rectangle and label
            thickness = 3 if confidence > 0.5 else 2
            cv2.rectangle(enhanced_frame, (x1, y1), (x2, y2), rect_color, thickness)
            
            # Calculate angle to PAPI light if we have GPS data
            angle_text = ""
            if cached_drone_data and reference_points:
                try:
                    drone_lat = cached_drone_data.get('latitude')
                    drone_lon = cached_drone_data.get('longitude') 
                    drone_alt = cached_drone_data.get('elevation', 0)
                    
                    # Get PAPI coordinates from reference points
                    papi_id = light_name.replace('PAPI_', '').lower()
                    papi_key = f"papi_{papi_id}"
                    
                    if (drone_lat and drone_lon and papi_key in reference_points and 
                        reference_points[papi_key].get('lat') and reference_points[papi_key].get('lon')):
                        
                        papi_lat = reference_points[papi_key]['lat']
                        papi_lon = reference_points[papi_key]['lon']
                        papi_alt = reference_points[papi_key].get('elevation', 0)
                        
                        # Calculate angle using our existing function
                        drone_data = {
                            'latitude': drone_lat,
                            'longitude': drone_lon,
                            'elevation': drone_alt
                        }
                        papi_gps = {
                            'lat': papi_lat,
                            'lon': papi_lon,
                            'elevation': papi_alt
                        }
                        
                        angle = calculate_angle(drone_data, papi_gps)
                        angle_text = f" {angle:.1f}Â°"
                        
                except Exception as e:
                    # If angle calculation fails, continue without angle
                    pass
            
            label = f"{light_name}{angle_text}"
            if confidence > 0:
                label += f" ({confidence:.2f})"
            
            # Add text label
            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            text_x = max(0, x1)
            text_y = max(text_size[1] + 5, y1 - 5)
            
            cv2.rectangle(enhanced_frame, 
                         (text_x, text_y - text_size[1] - 5), 
                         (text_x + text_size[0] + 5, text_y + 5), 
                         (0, 0, 0), -1)
            cv2.putText(enhanced_frame, label, (text_x + 2, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Add optimized overlays
        self._add_drone_position_overlay_optimized(enhanced_frame, frame_number, cached_drone_data, reference_points)
        self._add_progress_bar(enhanced_frame, frame_number, total_frames)
        
        return enhanced_frame
    
    def _add_drone_position_overlay_optimized(self, frame: np.ndarray, frame_number: int, 
                                             cached_drone_data: Dict = None,
                                             reference_points: Dict = None):
        """Optimized overlay using pre-computed GPS data"""
        height, width = frame.shape[:2]
        
        # Create semi-transparent overlay box
        overlay = frame.copy()
        box_height = 220
        box_width = 420
        
        cv2.rectangle(overlay, (10, 10), (box_width, box_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # Use cached drone data or fallback
        if cached_drone_data:
            drone_data = cached_drone_data
            gps_source = "ð¡ REAL GPS"
        else:
            raise ValueError(f"No GPS data available for frame {frame_number}. Video must contain GPS coordinates for processing.")
        
        # GPS quality indicators
        gps_quality = ""
        if drone_data.get('satellites'):
            gps_quality = f" ({drone_data['satellites']} sats)"
        if drone_data.get('accuracy'):
            gps_quality += f" Â±{drone_data['accuracy']:.1f}m"
        
        # Calculate angle to touch point if reference points available
        touch_point_angle_text = ""
        if reference_points and 'touch_point' in reference_points:
            try:
                drone_lat = drone_data.get('latitude')
                drone_lon = drone_data.get('longitude')
                drone_alt = drone_data.get('elevation', 0)
                
                touch_lat = reference_points['touch_point'].get('lat')
                touch_lon = reference_points['touch_point'].get('lon')
                touch_alt = reference_points['touch_point'].get('elevation', 0)
                
                if drone_lat and drone_lon and touch_lat and touch_lon:
                    # Calculate angle to touch point
                    touch_data = {
                        'latitude': drone_lat,
                        'longitude': drone_lon,
                        'elevation': drone_alt
                    }
                    touch_gps = {
                        'lat': touch_lat,
                        'lon': touch_lon,
                        'elevation': touch_alt
                    }
                    
                    touch_angle = calculate_angle(touch_data, touch_gps)
                    touch_point_angle_text = f"Touch Point: {touch_angle:.1f}Â°"
            except Exception as e:
                # If angle calculation fails, continue without angle
                pass

        # Basic drone data
        basic_texts = [
            f"Frame: {frame_number + 1} | {gps_source}{gps_quality}",
            f"Lat: {drone_data.get('latitude', 0):.6f}Â°",
            f"Lon: {drone_data.get('longitude', 0):.6f}Â°", 
            f"Alt: {drone_data.get('elevation', 0):.1f}m",
            f"Speed: {drone_data.get('speed', 0):.1f} m/s",
            f"Heading: {drone_data.get('heading', 0):.1f}Â°",
        ]
        
        # Add touch point angle if available
        if touch_point_angle_text:
            basic_texts.append(touch_point_angle_text)
        
        # Draw information
        text_color = (255, 255, 255)
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 2.25
        line_height = 80
        
        y_offset = 25
        for i, text in enumerate(basic_texts):
            y_pos = y_offset + i * line_height
            cv2.putText(frame, text, (20, y_pos), font, font_scale, text_color, 1)
    
    def _add_overlays_to_frame_with_tracking(self, frame: np.ndarray, tracked_positions: Dict, 
                                            frame_number: int, total_frames: int,
                                            measurements_data: List[Dict] = None,
                                            drone_telemetry: List[Dict] = None,
                                            reference_points: Dict = None,
                                            real_gps_data: List[GPSData] = None,
                                            fps: float = 30.0) -> np.ndarray:
        """Add drone position overlay and tracked PAPI light rectangles to frame"""
        enhanced_frame = frame.copy()
        height, width = frame.shape[:2]
        
        # Draw tracked PAPI light rectangles with current RGB values
        for light_name, pos in tracked_positions.items():
            if light_name not in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                continue
            
            # Skip if position data is incomplete
            if not isinstance(pos, dict) or 'x' not in pos or 'y' not in pos:
                logger.warning(f"Skipping {light_name}: incomplete position data: {pos}")
                continue
                
            pixel_x = pos['x']
            pixel_y = pos['y']
            pixel_size = max(20, pos.get('size', 20))
            
            # Calculate rectangle bounds
            half_size = pixel_size // 2
            x1 = max(0, pixel_x - half_size)
            y1 = max(0, pixel_y - half_size)
            x2 = min(width, pixel_x + half_size)
            y2 = min(height, pixel_y + half_size)
            
            # Use actual RGB values from detection for rectangle color
            rgb = pos.get('rgb', [255, 255, 255])
            confidence = pos.get('confidence', 0.0)
            
            # Convert RGB to BGR for OpenCV
            rect_color = (int(rgb[2]), int(rgb[1]), int(rgb[0]))  # BGR format
            
            # Draw rectangle around PAPI light with detected color
            thickness = 3 if confidence > 0.5 else 2  # Thicker line for higher confidence
            cv2.rectangle(enhanced_frame, (x1, y1), (x2, y2), rect_color, thickness)
            
            # Draw light name label
            label = f"{light_name}"
            if confidence > 0:
                label += f" ({confidence:.2f})"
            
            # Calculate text position (above the rectangle)
            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            text_x = max(0, x1)
            text_y = max(text_size[1] + 5, y1 - 5)
            
            # Draw text background
            cv2.rectangle(enhanced_frame, 
                         (text_x, text_y - text_size[1] - 5), 
                         (text_x + text_size[0] + 5, text_y + 5), 
                         (0, 0, 0), -1)
            
            # Draw text
            cv2.putText(enhanced_frame, label, (text_x + 2, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Keep the existing overlay methods for drone position, etc.
        self._add_drone_position_overlay(enhanced_frame, frame_number, drone_telemetry, reference_points, real_gps_data, fps)
        self._add_progress_bar(enhanced_frame, frame_number, total_frames)
        self._add_angle_overlay(enhanced_frame, frame_number, drone_telemetry, reference_points, real_gps_data, fps)
        
        return enhanced_frame

    def _add_overlays_to_frame(self, frame: np.ndarray, light_positions: Dict, 
                              frame_number: int, total_frames: int,
                              measurements_data: List[Dict] = None,
                              drone_telemetry: List[Dict] = None,
                              reference_points: Dict = None) -> np.ndarray:
        """Add drone position overlay and PAPI light rectangles to frame (legacy method)"""
        enhanced_frame = frame.copy()
        height, width = frame.shape[:2]
        
        # Draw PAPI light rectangles
        for light_name, pos in light_positions.items():
            # Convert percentage coordinates to pixel coordinates
            x_percent = pos.get("x", 0)
            y_percent = pos.get("y", 0)
            size_percent = pos.get("size", 8)
            
            pixel_x = int((x_percent / 100) * width)
            pixel_y = int((y_percent / 100) * height)
            pixel_size = max(20, int((size_percent / 100) * width))
            
            # Calculate rectangle bounds
            half_size = pixel_size // 2
            x1 = max(0, pixel_x - half_size)
            y1 = max(0, pixel_y - half_size)
            x2 = min(width, pixel_x + half_size)
            y2 = min(height, pixel_y + half_size)
            
            # Get light status color from measurements if available
            light_color = self._get_light_status_color(light_name, frame_number, measurements_data)
            
            # Draw rectangle around PAPI light
            cv2.rectangle(enhanced_frame, (x1, y1), (x2, y2), light_color, 3)
            
            # Draw center cross
            cv2.drawMarker(enhanced_frame, (pixel_x, pixel_y), light_color, 
                          cv2.MARKER_CROSS, 15, 2)
            
            # Add light name label
            label_pos = (x1, y1 - 10 if y1 > 20 else y2 + 25)
            cv2.putText(enhanced_frame, light_name, label_pos, 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, light_color, 2)
        
        # Add drone position overlay (legacy method - use fallback)
        self._add_drone_position_overlay(enhanced_frame, frame_number, drone_telemetry, reference_points, None, 30.0)
        
        # Add frame information overlay
        self._add_frame_info_overlay(enhanced_frame, frame_number, total_frames)
        
        return enhanced_frame
    
    def _get_light_status_color(self, light_name: str, frame_number: int, 
                              measurements_data: List[Dict] = None) -> Tuple[int, int, int]:
        """Get color based on light status from measurements"""
        if not measurements_data or frame_number >= len(measurements_data):
            return (0, 255, 255)  # Default yellow
        
        try:
            frame_data = measurements_data[frame_number]
            if light_name in frame_data:
                status = frame_data[light_name].get('status', 'not_visible')
                
                color_map = {
                    'red': (0, 0, 255),      # Red
                    'white': (255, 255, 255), # White
                    'transition': (0, 165, 255), # Orange
                    'not_visible': (128, 128, 128), # Gray
                }
                return color_map.get(status, (0, 255, 255))  # Default yellow
        except (IndexError, KeyError):
            pass
        
        return (0, 255, 255)  # Default yellow
    
    def _add_drone_position_overlay(self, frame: np.ndarray, frame_number: int, 
                                   drone_telemetry: List[Dict] = None,
                                   reference_points: Dict = None,
                                   real_gps_data: List[GPSData] = None,
                                   fps: float = 30.0):
        """Add drone position information overlay with angles to PAPI lights and touch point"""
        height, width = frame.shape[:2]
        
        # Create semi-transparent overlay box - expanded for more data
        overlay = frame.copy()
        box_height = 220  # Increased height for angle data
        box_width = 420   # Increased width for angle data
        
        # Position in top-left corner
        cv2.rectangle(overlay, (10, 10), (box_width, box_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # Get real drone data for this frame
        drone_data = None
        
        # Priority 1: Use real GPS data extracted from video file
        if real_gps_data:
            gps_extractor = GPSExtractor()
            interpolated_gps = gps_extractor.interpolate_gps_for_frame(real_gps_data, frame_number, fps)
            if interpolated_gps:
                drone_data = {
                    "latitude": interpolated_gps.latitude,
                    "longitude": interpolated_gps.longitude, 
                    "elevation": interpolated_gps.altitude,
                    "speed": interpolated_gps.speed or 0.0,
                    "heading": interpolated_gps.heading or 0.0,
                    "satellites": interpolated_gps.satellites,
                    "accuracy": interpolated_gps.accuracy
                }
                logger.debug(f"Frame {frame_number}: Using real GPS data - Lat: {interpolated_gps.latitude:.6f}, Lon: {interpolated_gps.longitude:.6f}, Alt: {interpolated_gps.altitude:.1f}m")
        
        # Priority 2: Use provided drone telemetry
        if not drone_data and drone_telemetry and frame_number < len(drone_telemetry):
            drone_data = drone_telemetry[frame_number]
            logger.debug(f"Frame {frame_number}: Using provided telemetry data")
        
        # Priority 3: Fail if no GPS data is available
        if not drone_data:
            raise ValueError(f"No GPS data available for frame {frame_number}. Video must contain GPS coordinates for processing.")
        
        # Calculate angles to PAPI lights and touch point
        angles_data = self._calculate_angles_to_targets(drone_data, reference_points)
        
        # Draw drone position information
        text_color = (255, 255, 255)
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 2.25
        line_height = 80
        
        # Basic drone data with GPS quality indicators
        gps_source = "ð¡ REAL GPS" if real_gps_data else "â ï¸ FALLBACK"
        gps_quality = ""
        if drone_data.get('satellites'):
            gps_quality = f" ({drone_data['satellites']} sats)"
        if drone_data.get('accuracy'):
            gps_quality += f" Â±{drone_data['accuracy']:.1f}m"
        
        basic_texts = [
            f"Frame: {frame_number + 1} | {gps_source}{gps_quality}",
            f"Lat: {drone_data.get('latitude', 0):.6f}Â°",
            f"Lon: {drone_data.get('longitude', 0):.6f}Â°", 
            f"Alt: {drone_data.get('elevation', 0):.1f}m",
            f"Speed: {drone_data.get('speed', 0):.1f} m/s",
            f"Heading: {drone_data.get('heading', 0):.1f}Â°",
            "",  # Empty line separator
            "ð Angles to Targets:"
        ]
        
        # Add basic information
        y_offset = 25
        for i, text in enumerate(basic_texts):
            y_pos = y_offset + i * line_height
            cv2.putText(frame, text, (20, y_pos), font, font_scale, text_color, 1)
        
        # Add angle information with color coding
        y_offset += len(basic_texts) * line_height + 5
        
        # PAPI light angles
        papi_colors = {
            'PAPI_A': (0, 255, 255),    # Yellow
            'PAPI_B': (255, 165, 0),    # Orange  
            'PAPI_C': (255, 0, 255),    # Magenta
            'PAPI_D': (0, 255, 0)       # Green
        }
        
        for i, (papi_id, angle_data) in enumerate(angles_data.items()):
            if papi_id.startswith('PAPI_'):
                color = papi_colors.get(papi_id, (255, 255, 255))
                angle_text = f"{papi_id}: {angle_data['angle']:.1f}Â° ({angle_data['distance']:.0f}m)"
                y_pos = y_offset + i * line_height
                cv2.putText(frame, angle_text, (25, y_pos), font, font_scale, color, 1)
        
        # Touch point angle (if available)
        if 'touch_point' in angles_data:
            touch_data = angles_data['touch_point']
            touch_text = f"Touch Pt: {touch_data['angle']:.1f}Â° ({touch_data['distance']:.0f}m)"
            y_pos = y_offset + len([k for k in angles_data.keys() if k.startswith('PAPI_')]) * line_height
            cv2.putText(frame, touch_text, (25, y_pos), font, font_scale, (255, 255, 255), 1)
    
    def _calculate_angles_to_targets(self, drone_data: Dict, reference_points: Dict = None) -> Dict:
        """Calculate angles from drone to all target points (PAPI lights and touch point)"""
        angles_data = {}
        
        if not reference_points:
            # Default mock reference points for TrenÄÃ­n airport (official LZTN coordinates)
            reference_points = {
                'PAPI_A': {'latitude': 48.85959167, 'longitude': 17.98482778, 'elevation': 245.592},
                'PAPI_B': {'latitude': 48.85957750, 'longitude': 17.98489444, 'elevation': 245.587},
                'PAPI_C': {'latitude': 48.85956389, 'longitude': 17.98495000, 'elevation': 245.594},
                'PAPI_D': {'latitude': 48.85955028, 'longitude': 17.98500833, 'elevation': 245.615},
                'touch_point': {'latitude': 48.86111111, 'longitude': 17.98541667, 'elevation': 242.0}
            }
        
        drone_lat = drone_data.get('latitude')
        drone_lon = drone_data.get('longitude') 
        drone_alt = drone_data.get('elevation')
        
        # Validate that all required coordinates are present
        if drone_lat is None or drone_lon is None or drone_alt is None:
            raise ValueError(f"Incomplete GPS data in frame {frame_number}. Missing: " +
                           f"{'latitude ' if drone_lat is None else ''}" +
                           f"{'longitude ' if drone_lon is None else ''}" +
                           f"{'elevation' if drone_alt is None else ''}")
        
        for target_id, target_pos in reference_points.items():
            target_lat = target_pos.get('latitude')
            target_lon = target_pos.get('longitude')
            target_elevation = target_pos.get('elevation', 0)
            
            if target_lat is not None and target_lon is not None:
                # Calculate ground distance using Haversine formula
                ground_dist = haversine_distance(drone_lat, drone_lon, target_lat, target_lon)
                
                # Calculate height difference
                height_diff = drone_alt - target_elevation
                
                # Calculate angle (elevation angle from horizontal)
                if ground_dist > 0:
                    angle = np.degrees(np.arctan(height_diff / ground_dist))
                else:
                    angle = 90.0 if height_diff > 0 else -90.0
                
                # Calculate direct distance
                direct_distance = math.sqrt(ground_dist**2 + height_diff**2)
                
                angles_data[target_id] = {
                    'angle': angle,
                    'distance': direct_distance,
                    'ground_distance': ground_dist,
                    'height_diff': height_diff
                }
            else:
                # Fallback data if coordinates not available
                angles_data[target_id] = {
                    'angle': 0.0,
                    'distance': 500.0,
                    'ground_distance': 500.0,
                    'height_diff': 0.0
                }
        
        return angles_data
    
    def _add_frame_info_overlay(self, frame: np.ndarray, frame_number: int, total_frames: int):
        """Add frame information overlay"""
        height, width = frame.shape[:2]
        
        # Progress bar
        bar_width = 300
        bar_height = 8
        bar_x = width - bar_width - 20
        bar_y = height - 40
        
        # Background bar
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), 
                     (50, 50, 50), -1)
        
        # Progress bar
        progress = frame_number / max(1, total_frames - 1)
        progress_width = int(bar_width * progress)
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + progress_width, bar_y + bar_height), 
                     (0, 255, 0), -1)
        
        # Frame counter
        frame_text = f"{frame_number + 1}/{total_frames}"
        cv2.putText(frame, frame_text, (bar_x, bar_y - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 255, 255), 3)
    
    def generate_papi_videos(self, video_path: str, session_id: str, light_positions: Dict) -> Dict[str, str]:
        """Generate individual videos for each PAPI light using tracking"""
        video_paths = {}
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                logger.error(f"Failed to open video: {video_path}")
                return video_paths
            
            # Get video properties
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # Initialize PAPI light tracker for individual videos
            light_tracker = PAPILightTracker(light_positions, frame_width, frame_height)
            
            # Create video writers for each PAPI light
            video_writers = {}
            light_regions = {}
            
            for light_name in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                if light_name in light_positions:
                    # Initial region (will be updated by tracker)
                    pos = light_positions[light_name]
                    x_pct = pos['x']
                    y_pct = pos['y']
                    width_pct = pos.get('width', 2)  # Use actual detected width
                    height_pct = pos.get('height', 2)  # Use actual detected height
                    
                    x = int((x_pct / 100) * frame_width)
                    y = int((y_pct / 100) * frame_height)
                    light_width = int((width_pct / 100) * frame_width)
                    light_height = int((height_pct / 100) * frame_height)
                    
                    # Set region to capture light with minimal padding (will be resized to 100x100)
                    padding = max(light_width, light_height, 20)  # Use light size or minimum 20px padding
                    x1 = max(0, x - padding)
                    y1 = max(0, y - padding)
                    x2 = min(frame_width, x + padding)
                    y2 = min(frame_height, y + padding)
                    
                    light_regions[light_name] = (x1, y1, x2, y2)
                    
                    # Create output video path
                    video_filename = f"{session_id}_{light_name.lower()}_light.mp4"
                    video_output_path = os.path.join(self.output_dir, video_filename)
                    video_paths[light_name] = video_output_path
                    
                    # Create video writer with H.264 codec for browser compatibility
                    fourcc = cv2.VideoWriter_fourcc(*'avc1')  # H.264 codec
                    video_writers[light_name] = cv2.VideoWriter(
                        video_output_path, fourcc, fps, (300, 350)  # Fixed 300x350 output size (50px for footer)
                    )
            
            frame_count = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Update light positions using tracker
                tracked_positions = light_tracker.update_frame(frame, frame_count)
                
                # Extract regions for each PAPI light using tracked positions
                for light_name in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                    if light_name in video_writers and light_name in tracked_positions:
                        tracked_pos = tracked_positions[light_name]
                        
                        # Get tracked position and size
                        x = tracked_pos['x']
                        y = tracked_pos['y']
                        size = tracked_pos['size']
                        
                        # Calculate region with padding
                        padding = size * 2
                        x1 = max(0, x - padding)
                        y1 = max(0, y - padding)
                        x2 = min(frame_width, x + padding)
                        y2 = min(frame_height, y + padding)
                        
                        # Update the region size if needed (resize video writer if dimensions changed significantly)
                        original_x1, original_y1, original_x2, original_y2 = light_regions[light_name]
                        new_width = x2 - x1
                        new_height = y2 - y1
                        original_width = original_x2 - original_x1
                        original_height = original_y2 - original_y1
                        
                        # If size changed significantly, keep original size but center on new position
                        if abs(new_width - original_width) > 20 or abs(new_height - original_height) > 20:
                            # Keep original size, just center on new position
                            half_width = original_width // 2
                            half_height = original_height // 2
                            x1 = max(0, x - half_width)
                            y1 = max(0, y - half_height)
                            x2 = min(frame_width, x1 + original_width)
                            y2 = min(frame_height, y1 + original_height)
                            
                            # Adjust if hitting boundaries
                            if x2 == frame_width:
                                x1 = x2 - original_width
                            if y2 == frame_height:
                                y1 = y2 - original_height
                        
                        # Extract the region
                        light_frame = frame[y1:y2, x1:x2]
                        
                        if light_frame.size > 0:
                            # Get RGB values for color-coded text
                            rgb = tracked_pos.get('rgb', [255, 255, 255])
                            
                            # Resize the light region to 300x300
                            light_frame_resized = cv2.resize(light_frame, (300, 300))
                            
                            # Create final frame with white footer strip
                            final_frame = np.zeros((350, 300, 3), dtype=np.uint8)
                            
                            # Copy the resized light frame to the top part
                            final_frame[0:300, 0:300] = light_frame_resized
                            
                            # Create white footer strip (50px height)
                            final_frame[300:350, 0:300] = [255, 255, 255]  # White background
                            
                            # Add text to white footer strip
                            confidence = tracked_pos.get('confidence', 0.0)
                            
                            # Light name and frame info
                            info_text = f"{light_name} - Frame {frame_count}"
                            if confidence > 0:
                                info_text += f" ({confidence:.2f})"
                            
                            cv2.putText(final_frame, info_text, (5, 320), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 0), 3)  # Black text on white
                            
                            # Add color-coded RGB values in footer
                            y_pos = 340
                            font = cv2.FONT_HERSHEY_SIMPLEX
                            font_scale = 3.0
                            thickness = 3
                            
                            # Red component
                            cv2.putText(final_frame, f"R:{rgb[0]:.0f}", (5, y_pos), 
                                      font, font_scale, (0, 0, 255), thickness)  # Red color (BGR)
                            
                            # Green component  
                            cv2.putText(final_frame, f"G:{rgb[1]:.0f}", (80, y_pos), 
                                      font, font_scale, (0, 255, 0), thickness)  # Green color (BGR)
                            
                            # Blue component
                            cv2.putText(final_frame, f"B:{rgb[2]:.0f}", (155, y_pos), 
                                      font, font_scale, (255, 0, 0), thickness)  # Blue color (BGR)
                            
                            video_writers[light_name].write(final_frame)
                
                frame_count += 1
                
                # Progress logging every 100 frames
                if frame_count % 100 == 0:
                    logger.info(f"Processed {frame_count} frames for PAPI videos")
            
            # Release resources
            cap.release()
            for writer in video_writers.values():
                writer.release()
            
            logger.info(f"Generated {len(video_paths)} PAPI light videos")
            return video_paths
            
        except Exception as e:
            logger.error(f"Error generating PAPI videos: {e}")
            return {}
    
    def _add_progress_bar(self, frame: np.ndarray, frame_number: int, total_frames: int):
        """Add progress bar overlay to frame"""
        # Fixed indentation
        height, width = frame.shape[:2]
        
        # Progress bar settings
        bar_width = 300
        bar_height = 8
        bar_x = width - bar_width - 20
        bar_y = height - 40
        
        # Background bar
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), 
                     (50, 50, 50), -1)
        
        # Progress bar
        progress = frame_number / max(1, total_frames - 1)
        progress_width = int(bar_width * progress)
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + progress_width, bar_y + bar_height), 
                     (0, 255, 0), -1)
        
        # Frame counter
        frame_text = f"{frame_number + 1}/{total_frames}"
        cv2.putText(frame, frame_text, (bar_x, bar_y - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 255, 255), 3)
    
    
    def _add_angle_overlay(self, frame: np.ndarray, frame_number: int, 
                          drone_telemetry: List[Dict] = None,
                          reference_points: Dict = None,
                          real_gps_data: List[GPSData] = None,
                          fps: float = 30.0):
        """Add angle information overlay to frame"""
        height, width = frame.shape[:2]
        
        # Get real drone data for this frame (same logic as main overlay)
        drone_data = None
        
        # Priority 1: Use real GPS data extracted from video file
        if real_gps_data:
            gps_extractor = GPSExtractor()
            interpolated_gps = gps_extractor.interpolate_gps_for_frame(real_gps_data, frame_number, fps)
            if interpolated_gps:
                drone_data = {
                    "latitude": interpolated_gps.latitude,
                    "longitude": interpolated_gps.longitude, 
                    "altitude": interpolated_gps.altitude,
                    "speed": interpolated_gps.speed or 0.0,
                    "heading": interpolated_gps.heading or 0.0
                }
        
        # Priority 2: Use provided drone telemetry
        if not drone_data and drone_telemetry and frame_number < len(drone_telemetry):
            drone_data = drone_telemetry[frame_number]
        
        # Priority 3: Fail if no GPS data is available  
        if not drone_data:
            raise ValueError(f"No GPS data available for frame {frame_number}. Video must contain GPS coordinates for processing.")
        
        # Calculate angles to PAPI lights and touch point if reference points available
        if reference_points:
            try:
                # Position angle info in the top-right corner
                info_x = width - 250
                info_y = 30
                line_height = 25
                
                # Semi-transparent background
                overlay = frame.copy()
                cv2.rectangle(overlay, (info_x - 10, info_y - 10), 
                             (width - 10, info_y + 5 * line_height), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
                
                # Calculate and display angles
                y_pos = info_y
                for papi_name in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                    if papi_name in reference_points:
                        # Simple angle calculation (in real implementation, would use proper geodesic calculations)
                        papi_point = reference_points[papi_name]
                        # Use proper Haversine distance calculation for accurate angles
                        ground_dist = haversine_distance(
                            drone_data['latitude'], drone_data['longitude'],
                            papi_point.get('latitude'), papi_point.get('longitude')
                        )
                        alt_diff = drone_data['altitude'] - papi_point.get('elevation')
                        
                        # Correct angle calculation using proper ground distance
                        if ground_dist > 0:
                            angle = np.degrees(np.arctan(alt_diff / ground_dist))
                        else:
                            angle = 90.0 if alt_diff > 0 else -90.0
                        
                        angle_text = f"{papi_name}: {angle:.1f}Â°"
                        cv2.putText(frame, angle_text, (info_x, y_pos), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                        y_pos += line_height
                
                # Touch point angle if available
                if 'TOUCH_POINT' in reference_points:
                    touch_point = reference_points['TOUCH_POINT']
                    # Use proper Haversine distance calculation for accurate angles
                    ground_dist = haversine_distance(
                        drone_data['latitude'], drone_data['longitude'],
                        touch_point.get('latitude'), touch_point.get('longitude')
                    )
                    alt_diff = drone_data['altitude'] - touch_point.get('elevation')
                    
                    # Correct angle calculation using proper ground distance
                    if ground_dist > 0:
                        angle = np.degrees(np.arctan(alt_diff / ground_dist))
                    else:
                        angle = 90.0 if alt_diff > 0 else -90.0
                    
                    angle_text = f"Touch Point: {angle:.1f}Â°"
                    cv2.putText(frame, angle_text, (info_x, y_pos), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
            except Exception as e:
                logger.warning(f"Error calculating angles: {e}")
        else:
            # Display placeholder when no reference points
            info_x = width - 200
            info_y = 30
            cv2.putText(frame, "Angles: No ref points", (info_x, info_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (128, 128, 128), 2)


class PAPIReportGenerator:
    """Generate comprehensive HTML reports for PAPI measurements"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def generate_html_report(self, session_data: Dict, measurements: List[Dict], video_paths: Dict[str, str] = None, reference_points: Dict = None, enhanced_main_video_path: str = None) -> str:
        """Generate interactive HTML report with Plotly charts, embedded videos, and reference point coordinates"""
        try:
            # Organize data by PAPI light
            papi_data = {}
            timestamps = []
            
            for measurement in measurements:
                timestamp = measurement.get('timestamp', 0)
                timestamps.append(timestamp / 1000)  # Convert to seconds
                
                for papi_id in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
                    if papi_id not in papi_data:
                        papi_data[papi_id] = {
                            'r_values': [],
                            'g_values': [],
                            'b_values': [],
                            'status': [],
                            'intensity': [],
                            'angles': [],
                            'ground_distance': [],
                            'direct_distance': []
                        }
                    
                    if papi_id in measurement:
                        data = measurement[papi_id]
                        rgb = data.get('rgb', {'r': 0, 'g': 0, 'b': 0})
                        
                        papi_data[papi_id]['r_values'].append(rgb['r'])
                        papi_data[papi_id]['g_values'].append(rgb['g'])
                        papi_data[papi_id]['b_values'].append(rgb['b'])
                        papi_data[papi_id]['status'].append(data.get('status', 'not_visible'))
                        papi_data[papi_id]['intensity'].append(data.get('intensity', 0))
                        papi_data[papi_id]['angles'].append(data.get('angle', 0))
                        papi_data[papi_id]['ground_distance'].append(data.get('distance_ground', 0))
                        papi_data[papi_id]['direct_distance'].append(data.get('distance_direct', 0))
            
            # Generate HTML with interactive charts and videos
            html_content = self._create_html_template(
                session_data, papi_data, timestamps, video_paths or {}, 
                reference_points or {}, enhanced_main_video_path
            )
            
            # Save report
            report_filename = f"papi_report_{session_data.get('session_id', 'unknown')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            report_path = os.path.join(self.output_dir, report_filename)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"HTML report generated: {report_path}")
            return report_path
            
        except Exception as e:
            logger.error(f"Error generating HTML report: {e}")
            return ""
    
    def _create_html_template(self, session_data: Dict, papi_data: Dict, timestamps: List[float], video_paths: Dict[str, str], reference_points: Dict, enhanced_main_video_path: str = None) -> str:
        """Create HTML template with Plotly charts, embedded videos, and reference point coordinates"""
        
        # Create individual charts for each PAPI light
        charts_html = ""
        
        for papi_id in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
            if papi_id in papi_data and papi_data[papi_id]['r_values']:
                data = papi_data[papi_id]
                
                # Create RGB + Angle chart
                fig = make_subplots(
                    rows=2, cols=1,
                    subplot_titles=(f'{papi_id} RGB Values', f'{papi_id} Angle & Distances'),
                    vertical_spacing=0.1
                )
                
                # RGB traces
                fig.add_trace(go.Scatter(x=timestamps, y=data['r_values'], name='Red', 
                                       line=dict(color='red')), row=1, col=1)
                fig.add_trace(go.Scatter(x=timestamps, y=data['g_values'], name='Green',
                                       line=dict(color='green')), row=1, col=1)
                fig.add_trace(go.Scatter(x=timestamps, y=data['b_values'], name='Blue',
                                       line=dict(color='blue')), row=1, col=1)
                
                # Angle and distance traces
                fig.add_trace(go.Scatter(x=timestamps, y=data['angles'], name='Angle (degrees)',
                                       line=dict(color='purple')), row=2, col=1)
                fig.add_trace(go.Scatter(x=timestamps, y=data['ground_distance'], name='Ground Distance (m)',
                                       line=dict(color='orange')), row=2, col=1)
                
                fig.update_layout(
                    title=f'{papi_id} Analysis',
                    height=600,
                    showlegend=True
                )
                fig.update_xaxes(title_text="Time (seconds)")
                fig.update_yaxes(title_text="RGB Value", row=1, col=1)
                fig.update_yaxes(title_text="Angle (deg) / Distance (m)", row=2, col=1)
                
                chart_html = pyo.plot(fig, output_type='div', include_plotlyjs=False)
                
                # Add video player if video exists for this PAPI light
                video_html = ""
                if papi_id in video_paths and video_paths[papi_id]:
                    # Use API endpoint for video src with full hostname
                    light_name = papi_id.lower()  # papi_a, papi_b, etc.
                    # Import settings here to get the configured base URL
                    from app.core.config import settings
                    video_src = f"{settings.API_BASE_URL}/api/v1/papi-measurements/session/{session_data.get('session_id')}/papi-video/{light_name}"
                    video_html = f'''
                    <div class="video-container">
                        <h4>{papi_id} Light Video</h4>
                        <video width="400" height="300" controls>
                            <source src="{video_src}" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p class="video-description">
                            This video shows the {papi_id} light throughout the measurement flight.
                            The crosshair marks the light position and frame numbers are displayed.
                        </p>
                    </div>
                    '''
                
                # Add reference point coordinates if available
                coordinates_html = ""
                if papi_id in reference_points:
                    coords = reference_points[papi_id]
                    coordinates_html = f'''
                    <div class="coordinates-info">
                        <h5>ð Reference Point Coordinates</h5>
                        <div class="coord-grid">
                            <div class="coord-item">
                                <span class="coord-label">Latitude:</span>
                                <span class="coord-value">{coords.get('latitude', 'N/A')}Â°</span>
                            </div>
                            <div class="coord-item">
                                <span class="coord-label">Longitude:</span>
                                <span class="coord-value">{coords.get('longitude', 'N/A')}Â°</span>
                            </div>
                            <div class="coord-item">
                                <span class="coord-label">Elevation:</span>
                                <span class="coord-value">{coords.get('elevation', 'N/A')} m</span>
                            </div>
                        </div>
                    </div>
                    '''
                
                # Combine chart, video, and coordinates in a layout
                charts_html += f'''
                <div class="papi-section">
                    <h3>{papi_id} Analysis</h3>
                    {coordinates_html}
                    <div class="chart-video-container">
                        <div class="chart-container">{chart_html}</div>
                        {video_html}
                    </div>
                </div>
                '''
        
        # Create summary statistics
        summary_html = self._create_summary_table(papi_data, timestamps)
        
        # Create reference points summary (including touch point)
        reference_points_html = self._create_reference_points_table(reference_points)
        
        html_template = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>PAPI Measurement Report - {session_data.get('session_id', 'Unknown')}</title>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                .header {{ background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px; }}
                .papi-section {{ background: white; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                .chart-video-container {{ display: flex; gap: 20px; align-items: flex-start; }}
                .chart-container {{ flex: 2; min-width: 0; }}
                .video-container {{ flex: 1; min-width: 300px; }}
                .video-container h4 {{ margin-top: 0; color: #333; }}
                .video-container video {{ width: 100%; height: auto; border-radius: 8px; }}
                .video-description {{ font-size: 12px; color: #666; margin-top: 10px; }}
                .coordinates-info {{ background: #f8f9fa; padding: 15px; border-radius: 6px; margin-bottom: 20px; border-left: 4px solid #007bff; }}
                .coordinates-info h5 {{ margin: 0 0 10px 0; color: #333; font-size: 14px; }}
                .coord-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; }}
                .coord-item {{ display: flex; justify-content: space-between; padding: 5px 0; }}
                .coord-label {{ font-weight: bold; color: #666; }}
                .coord-value {{ color: #333; font-family: monospace; }}
                .summary {{ background: white; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                table {{ width: 100%; border-collapse: collapse; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: center; }}
                th {{ background-color: #4CAF50; color: white; }}
                .status-red {{ background-color: #ffebee; }}
                .status-white {{ background-color: #f3f3f3; }}
                .status-transition {{ background-color: #fff3e0; }}
                .status-not_visible {{ background-color: #fafafa; }}
                @media (max-width: 768px) {{
                    .chart-video-container {{ flex-direction: column; }}
                    .video-container {{ min-width: auto; }}
                }}
                .reference-points-section {{ background: white; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                .reference-points-section h3 {{ margin-top: 0; color: #333; font-size: 18px; }}
                .reference-table {{ width: 100%; border-collapse: collapse; margin-top: 15px; }}
                .reference-table th {{ background-color: #007bff; color: white; padding: 12px 8px; }}
                .reference-table td {{ padding: 10px 8px; border: 1px solid #ddd; }}
                .reference-table tr:nth-child(even) {{ background-color: #f8f9fa; }}
                .reference-table tr:hover {{ background-color: #e9ecef; }}
                .original-video-section {{ background: white; padding: 20px; border-radius: 8px; margin: 20px 0; }}
                .original-video-section h2 {{ margin-top: 0; color: #333; }}
                .original-video-container {{ text-align: center; }}
                .original-video-container video {{ border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ð©ï¸ PAPI Light Measurement Report</h1>
                <p><strong>Session ID:</strong> {session_data.get('session_id', 'Unknown')}</p>
                <p><strong>Airport:</strong> {session_data.get('airport_icao', 'Unknown')}</p>
                <p><strong>Runway:</strong> {session_data.get('runway_code', 'Unknown')}</p>
                <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p><strong>Total Frames Analyzed:</strong> {len(timestamps)}</p>
                <p><strong>Duration:</strong> {max(timestamps) - min(timestamps):.1f} seconds</p>
            </div>
            
            <div class="original-video-section">
                <h2>ð¥ Enhanced Analysis Video</h2>
                <div class="original-video-container">
                    <video width="800" height="600" controls>
                        <source src="{self._get_enhanced_video_url(session_data.get('session_id'), enhanced_main_video_path)}" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="video-description">
                        Enhanced video with real-time drone position information (latitude, longitude, elevation), 
                        PAPI light status rectangles, angles to each PAPI light and touch point, and frame progress. 
                        The rectangles change color based on light status: Red for red lights, White for white lights, 
                        Orange for transition, Gray for not visible. Angle measurements show elevation angles from 
                        drone to each target with distance information.
                    </p>
                </div>
            </div>
            
            <div class="summary">
                <h2>ð Summary Statistics</h2>
                {summary_html}
            </div>
            
            {reference_points_html}
            
            <div class="charts">
                <h2>ð Detailed Analysis</h2>
                {charts_html}
            </div>
        </body>
        </html>
        """
        
        return html_template
    
    def _get_enhanced_video_url(self, session_id: str, enhanced_main_video_path: str) -> str:
        """Get URL for enhanced video or fallback to original video"""
        from app.core.config import settings
        
        if enhanced_main_video_path and os.path.exists(enhanced_main_video_path):
            # Use enhanced video endpoint (we'll create this)
            return f"{settings.API_BASE_URL}/api/v1/papi-measurements/session/{session_id}/enhanced-video"
        else:
            # Fallback to original video
            return f"{settings.API_BASE_URL}/api/v1/papi-measurements/session/{session_id}/original-video"
    
    def _create_summary_table(self, papi_data: Dict, timestamps: List[float]) -> str:
        """Create summary statistics table"""
        summary_rows = ""
        
        for papi_id in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
            if papi_id in papi_data and papi_data[papi_id]['status']:
                data = papi_data[papi_id]
                
                # Calculate statistics
                status_counts = {}
                for status in data['status']:
                    status_counts[status] = status_counts.get(status, 0) + 1
                
                avg_intensity = sum(data['intensity']) / len(data['intensity']) if data['intensity'] else 0
                avg_angle = sum(data['angles']) / len(data['angles']) if data['angles'] else 0
                avg_distance = sum(data['ground_distance']) / len(data['ground_distance']) if data['ground_distance'] else 0
                
                summary_rows += f"""
                <tr>
                    <td><strong>{papi_id}</strong></td>
                    <td>{len(data['status'])}</td>
                    <td>{status_counts.get('red', 0)}</td>
                    <td>{status_counts.get('white', 0)}</td>
                    <td>{status_counts.get('transition', 0)}</td>
                    <td>{status_counts.get('not_visible', 0)}</td>
                    <td>{avg_intensity:.1f}</td>
                    <td>{avg_angle:.1f}Â°</td>
                    <td>{avg_distance:.1f}m</td>
                </tr>
                """
        
        return f"""
        <table>
            <tr>
                <th>PAPI Light</th>
                <th>Total Frames</th>
                <th>Red Frames</th>
                <th>White Frames</th>
                <th>Transition Frames</th>
                <th>Not Visible</th>
                <th>Avg Intensity</th>
                <th>Avg Angle</th>
                <th>Avg Distance</th>
            </tr>
            {summary_rows}
        </table>
        """

    def _create_reference_points_table(self, reference_points: Dict) -> str:
        """Create reference points table including PAPI lights and touch point coordinates"""
        if not reference_points:
            return '<p style="color: #666; font-style: italic;">No reference point data available.</p>'
        
        reference_rows = ""
        
        # Add PAPI light coordinates
        for papi_id in ['PAPI_A', 'PAPI_B', 'PAPI_C', 'PAPI_D']:
            if papi_id in reference_points:
                coords = reference_points[papi_id]
                lat = coords.get('latitude', 'N/A')
                lon = coords.get('longitude', 'N/A') 
                elev = coords.get('elevation', 'N/A')
                
                reference_rows += f"""
                <tr>
                    <td><strong>{papi_id}</strong></td>
                    <td>PAPI Light</td>
                    <td>{lat}Â°</td>
                    <td>{lon}Â°</td>
                    <td>{elev} m</td>
                </tr>
                """
        
        # Add touch point coordinates  
        if 'touch_point' in reference_points:
            coords = reference_points['touch_point']
            lat = coords.get('latitude', 'N/A')
            lon = coords.get('longitude', 'N/A')
            elev = coords.get('elevation', 'N/A')
            
            reference_rows += f"""
            <tr>
                <td><strong>Touch Point</strong></td>
                <td>Runway Touch Point</td>
                <td>{lat}Â°</td>
                <td>{lon}Â°</td>
                <td>{elev} m</td>
            </tr>
            """
        
        return f"""
        <div class="reference-points-section">
            <h3>ðºï¸ Reference Points & Coordinates</h3>
            <table class="reference-table">
                <thead>
                    <tr>
                        <th>Point Name</th>
                        <th>Type</th>
                        <th>Latitude</th>
                        <th>Longitude</th>
                        <th>Elevation</th>
                    </tr>
                </thead>
                <tbody>
                    {reference_rows}
                </tbody>
            </table>
        </div>
        """